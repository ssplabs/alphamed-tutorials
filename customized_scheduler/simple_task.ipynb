{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "19482319-e36b-4c95-b644-fb7e36de4c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import secrets\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "from alphafed.contractor.common import ContractEvent\n",
    "from alphafed.contractor.task_message_contractor import (\n",
    "    ApplySharedFileSendingDataEvent, TaskMessageContractor,\n",
    "    TaskMessageEventFactory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cc22c65f-c772-42e7-8001-476780d8711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CheckinEvent(ContractEvent):\n",
    "    \"\"\"An event of checkin for a specific task.\"\"\"\n",
    "\n",
    "    TYPE = 'checkin'\n",
    "\n",
    "    peer_id: str\n",
    "    nonce: str\n",
    "\n",
    "    @classmethod\n",
    "    def contract_to_event(cls, contract: dict) -> 'CheckinEvent':\n",
    "        event_type = contract.get('type')\n",
    "        peer_id = contract.get('peer_id')\n",
    "        nonce = contract.get('nonce')\n",
    "        assert event_type == cls.TYPE, f'合约类型错误: {event_type}'\n",
    "        assert peer_id and isinstance(peer_id, str), f'invalid peer_id: {peer_id}'\n",
    "        assert nonce or isinstance(nonce, str), f'invalid nonce: {nonce}'\n",
    "        return CheckinEvent(peer_id=peer_id, nonce=nonce)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CheckinResponseEvent(ContractEvent):\n",
    "    \"\"\"An event of responding checkin event.\"\"\"\n",
    "\n",
    "    TYPE = 'checkin_response'\n",
    "\n",
    "    round: int\n",
    "    aggregator: str\n",
    "    nonce: str\n",
    "\n",
    "    @classmethod\n",
    "    def contract_to_event(cls, contract: dict) -> 'CheckinResponseEvent':\n",
    "        event_type = contract.get('type')\n",
    "        round = contract.get('round')\n",
    "        aggregator = contract.get('aggregator')\n",
    "        nonce = contract.get('nonce')\n",
    "        assert event_type == cls.TYPE, f'合约类型错误: {event_type}'\n",
    "        assert isinstance(round, int) and round >= 0, f'invalid round: {round}'\n",
    "        assert aggregator and isinstance(aggregator, str), f'invalid aggregator: {aggregator}'\n",
    "        assert nonce and isinstance(nonce, str), f'invalid nonce: {nonce}'\n",
    "        return CheckinResponseEvent(round=round, aggregator=aggregator, nonce=nonce)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class StartRoundEvent(ContractEvent):\n",
    "    \"\"\"An event of starting a new round of training.\"\"\"\n",
    "\n",
    "    TYPE = 'start_round'\n",
    "\n",
    "    round: int\n",
    "    calculators: List[str]\n",
    "    aggregator: str\n",
    "\n",
    "    @classmethod\n",
    "    def contract_to_event(cls, contract: dict) -> 'StartRoundEvent':\n",
    "        event_type = contract.get('type')\n",
    "        round = contract.get('round')\n",
    "        calculators = contract.get('calculators')\n",
    "        aggregator = contract.get('aggregator')\n",
    "        assert event_type == cls.TYPE, f'合约类型错误: {event_type}'\n",
    "        assert isinstance(round, int) and round > 0, f'invalid round: {round}'\n",
    "        assert (\n",
    "            calculators and isinstance(calculators, list)\n",
    "            and all(_peer_id and isinstance(_peer_id, str) for _peer_id in calculators)\n",
    "        ), f'invalid participants: {calculators}'\n",
    "        assert aggregator and isinstance(aggregator, str), f'invalid aggregator: {aggregator}'\n",
    "        return StartRoundEvent(round=round,\n",
    "                               calculators=calculators,\n",
    "                               aggregator=aggregator)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ReadyForAggregationEvent(ContractEvent):\n",
    "    \"\"\"An event of notifying that the aggregator is ready for aggregation.\"\"\"\n",
    "\n",
    "    TYPE = 'ready_for_aggregation'\n",
    "\n",
    "    round: int\n",
    "\n",
    "    @classmethod\n",
    "    def contract_to_event(cls, contract: dict) -> 'ReadyForAggregationEvent':\n",
    "        event_type = contract.get('type')\n",
    "        round = contract.get('round')\n",
    "        assert event_type == cls.TYPE, f'合约类型错误: {event_type}'\n",
    "        assert isinstance(round, int) and round > 0, f'invalid round: {round}'\n",
    "        return ReadyForAggregationEvent(round=round)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CloseRoundEvent(ContractEvent):\n",
    "    \"\"\"An event of closing a specific round of training.\"\"\"\n",
    "\n",
    "    TYPE = 'close_round'\n",
    "\n",
    "    round: int\n",
    "\n",
    "    @classmethod\n",
    "    def contract_to_event(cls, contract: dict) -> 'CloseRoundEvent':\n",
    "        event_type = contract.get('type')\n",
    "        round = contract.get('round')\n",
    "        assert event_type == cls.TYPE, f'合约类型错误: {event_type}'\n",
    "        assert isinstance(round, int) and round > 0, f'invalid round: {round}'\n",
    "        return CloseRoundEvent(round=round)\n",
    "\n",
    "\n",
    "UploadTrainingResultsEvent = ApplySharedFileSendingDataEvent\n",
    "DistributeParametersEvent = ApplySharedFileSendingDataEvent\n",
    "\n",
    "\n",
    "class SimpleFedAvgEventFactory(TaskMessageEventFactory):\n",
    "\n",
    "    _CLASS_MAP = {\n",
    "        CheckinEvent.TYPE: CheckinEvent,\n",
    "        CheckinResponseEvent.TYPE: CheckinResponseEvent,\n",
    "        StartRoundEvent.TYPE: StartRoundEvent,\n",
    "        ReadyForAggregationEvent.TYPE: ReadyForAggregationEvent,\n",
    "        CloseRoundEvent.TYPE: CloseRoundEvent,\n",
    "        **TaskMessageEventFactory._CLASS_MAP\n",
    "    }\n",
    "\n",
    "\n",
    "class SimpleFedAvgContractor(TaskMessageContractor):\n",
    "\n",
    "    def __init__(self, task_id: str):\n",
    "        super().__init__(task_id=task_id)\n",
    "        self._event_factory = SimpleFedAvgEventFactory\n",
    "\n",
    "    def checkin(self, peer_id: str) -> str:\n",
    "        \"\"\"Checkin to the task.\n",
    "\n",
    "        :return\n",
    "            A nonce string used for identifying matched sync_state reply.\n",
    "        \"\"\"\n",
    "        nonce = secrets.token_hex(16)\n",
    "        event = CheckinEvent(peer_id=peer_id, nonce=nonce)\n",
    "        self._new_contract(targets=self.EVERYONE, event=event)\n",
    "        return nonce\n",
    "\n",
    "    def respond_check_in(self,\n",
    "                         round: int,\n",
    "                         aggregator: str,\n",
    "                         nonce: str,\n",
    "                         requester_id: str):\n",
    "        \"\"\"Respond checkin event.\"\"\"\n",
    "        event = CheckinResponseEvent(round=round, aggregator=aggregator, nonce=nonce)\n",
    "        self._new_contract(targets=[requester_id], event=event)\n",
    "\n",
    "    def start_round(self,\n",
    "                    calculators: List[str],\n",
    "                    round: int,\n",
    "                    aggregator: str):\n",
    "        \"\"\"Create a round of training.\"\"\"\n",
    "        event = StartRoundEvent(round=round,\n",
    "                                calculators=calculators,\n",
    "                                aggregator=aggregator)\n",
    "        self._new_contract(targets=self.EVERYONE, event=event)\n",
    "\n",
    "    def notify_ready_for_aggregation(self, round: int):\n",
    "        \"\"\"Notify all that the aggregator is ready for aggregation.\"\"\"\n",
    "        event = ReadyForAggregationEvent(round=round)\n",
    "        self._new_contract(targets=self.EVERYONE, event=event)\n",
    "\n",
    "    def close_round(self, round: int):\n",
    "        \"\"\"Start a round of training.\"\"\"\n",
    "        event = CloseRoundEvent(round=round)\n",
    "        self._new_contract(targets=self.EVERYONE, event=event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "861f54b6-40a5-4947-9385-8769510623d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "from abc import ABCMeta, abstractmethod\n",
    "from typing import Dict, Tuple\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import torch\n",
    "from alphafed import get_result_dir, logger\n",
    "from alphafed.data_channel.shared_file_data_channel import \\\n",
    "    SharedFileDataChannel\n",
    "from alphafed.scheduler import Scheduler\n",
    "from torch import Tensor\n",
    "from torch.nn import Module\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9a3c7e10-e948-436a-b26e-36b0d62d54b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleFedAvgScheduler(Scheduler, metaclass=ABCMeta):\n",
    "    \"\"\"A simple FedAvg implementation as an example of customized scheduler.\"\"\"\n",
    "\n",
    "    _INIT = 'init'\n",
    "    _GETHORING = 'gethering'\n",
    "    _READY = 'ready'\n",
    "    _IN_A_ROUND = 'in_a_round'\n",
    "    _UPDATING = 'updating'\n",
    "    _CALCULATING = 'calculating'\n",
    "    _WAIT_FOR_AGGR = 'wait_4_aggr'\n",
    "    _AGGREGATING = 'aggregating'\n",
    "    _PERSISTING = 'persisting'\n",
    "    _CLOSING_ROUND = 'closing_round'\n",
    "    _FINISHING = 'finishing'\n",
    "\n",
    "    def __init__(self, clients: int, rounds: int):\n",
    "        \"\"\"Init.\n",
    "\n",
    "        Args:\n",
    "            clients:\n",
    "                The number of calculators.\n",
    "            rounds:\n",
    "                The number of training rounds.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.state = self._INIT\n",
    "\n",
    "        self.clients = clients\n",
    "        self.rounds = rounds\n",
    "\n",
    "        self._participants = []\n",
    "\n",
    "    @abstractmethod\n",
    "    def build_model(self) -> Module:\n",
    "        \"\"\"Return a model object which will be used for training.\"\"\"\n",
    "\n",
    "    @property\n",
    "    def model(self) -> Module:\n",
    "        \"\"\"Get the model object which is used for training.\"\"\"\n",
    "        if not hasattr(self, '_model'):\n",
    "            self._model = self.build_model()\n",
    "        return self._model\n",
    "\n",
    "    @abstractmethod\n",
    "    def build_optimizer(self, model: Module) -> Optimizer:\n",
    "        \"\"\"Return a optimizer object which will be used for training.\n",
    "\n",
    "        Args:\n",
    "            model:\n",
    "                The model object which is used for training.\n",
    "        \"\"\"\n",
    "\n",
    "    @property\n",
    "    def optimizer(self) -> Optimizer:\n",
    "        \"\"\"Get the optimizer object which is used for training.\"\"\"\n",
    "        if not hasattr(self, '_optimizer'):\n",
    "            self._optimizer = self.build_optimizer(model=self.model)\n",
    "        return self._optimizer\n",
    "\n",
    "    @abstractmethod\n",
    "    def build_train_dataloader(self) -> DataLoader:\n",
    "        \"\"\"Define the training dataloader.\n",
    "\n",
    "        You can transform the dataset, do some preprocess to the dataset.\n",
    "\n",
    "        Return:\n",
    "            training dataloader\n",
    "        \"\"\"\n",
    "\n",
    "    @property\n",
    "    def train_loader(self) -> DataLoader:\n",
    "        \"\"\"Get the training dataloader object.\"\"\"\n",
    "        if not hasattr(self, '_train_loader'):\n",
    "            self._train_loader = self.build_train_dataloader()\n",
    "        return self._train_loader\n",
    "\n",
    "    @abstractmethod\n",
    "    def build_test_dataloader(self) -> DataLoader:\n",
    "        \"\"\"Define the testing dataloader.\n",
    "\n",
    "        You can transform the dataset, do some preprocess to the dataset. If you do not\n",
    "        want to do testing after training, simply make it return None.\n",
    "\n",
    "        Args:\n",
    "            dataset:\n",
    "                training dataset\n",
    "        Return:\n",
    "            testing dataloader\n",
    "        \"\"\"\n",
    "\n",
    "    @property\n",
    "    def test_loader(self) -> DataLoader:\n",
    "        \"\"\"Get the testing dataloader object.\"\"\"\n",
    "        if not hasattr(self, '_test_loader'):\n",
    "            self._test_loader = self.build_test_dataloader()\n",
    "        return self._test_loader\n",
    "\n",
    "    @abstractmethod\n",
    "    def state_dict(self) -> Dict[str, Tensor]:\n",
    "        \"\"\"Get the params that need to train and update.\n",
    "\n",
    "        Only the params returned by this function will be updated and saved during aggregation.\n",
    "\n",
    "        Return:\n",
    "            List[Tensor], The list of model params.\n",
    "        \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_state_dict(self, state_dict: Dict[str, Tensor]):\n",
    "        \"\"\"Load the params that trained and updated.\n",
    "\n",
    "        Only the params returned by state_dict() should be loaded by this function.\n",
    "        \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def train_an_epoch(self):\n",
    "        \"\"\"Define the training steps in an epoch.\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def test(self):\n",
    "        \"\"\"Define the testing steps.\n",
    "\n",
    "        If you do not want to do testing after training, simply make it pass.\n",
    "        \"\"\"\n",
    "\n",
    "    def _setup_context(self, id: str, task_id: str, is_initiator: bool = False):\n",
    "        assert id, 'must specify a unique id for every participant'\n",
    "        assert task_id, 'must specify a task_id for every participant'\n",
    "\n",
    "        self.id = id\n",
    "        self.task_id = task_id\n",
    "        self._result_dir = get_result_dir(self.task_id)\n",
    "        self._log_dir = os.path.join(self._result_dir, 'tb_logs')\n",
    "        self.tb_writer = SummaryWriter(log_dir=self._log_dir)\n",
    "\n",
    "        self.is_initiator = is_initiator\n",
    "\n",
    "        self.contractor = SimpleFedAvgContractor(task_id=task_id)\n",
    "        self.data_channel = SharedFileDataChannel(self.contractor)\n",
    "        self.model\n",
    "        self.optimizer\n",
    "        self.round = 0\n",
    "\n",
    "    def _run(self, id: str, task_id: str, is_initiator: bool = False, recover: bool = False):\n",
    "        self._setup_context(id=id, task_id=task_id, is_initiator=is_initiator)\n",
    "        self.push_log(message='Local context is ready.')\n",
    "        try:\n",
    "            if self.is_initiator and recover:\n",
    "                self._recover_progress()\n",
    "            else:\n",
    "                self._clean_progress()\n",
    "            self._launch_process()\n",
    "        except Exception:\n",
    "            # 将错误信息推送到 Playground 前端界面，有助于了解错误原因并修正\n",
    "            err_stack = '\\n'.join(traceback.format_exception(*sys.exc_info()))\n",
    "            self.push_log(err_stack)\n",
    "\n",
    "    def _recover_progress(self):\n",
    "        \"\"\"Try to recover and continue from last running.\"\"\"\n",
    "        # 如果上一次执行计算任务因为某些偶发原因失败了。在排除故障原因后，希望能够从失败的地方\n",
    "        # 恢复计算进度继续计算，而不是重新开始，可以在这里提供恢复进度的处理逻辑。\n",
    "        pass\n",
    "\n",
    "    def _clean_progress(self):\n",
    "        \"\"\"Clean existing progress data.\"\"\"\n",
    "        # 如果曾经执行过计算任务，在计算环境中留下了一些过往的痕迹。现在想要从头开始重新运行计算\n",
    "        # 任务，但是残留的数据可能会干扰当前这一次运行，可以在这里提供清理环境的处理逻辑。\n",
    "        pass\n",
    "\n",
    "    def _launch_process(self):\n",
    "        self.push_log(f'Node {self.id} is up.')\n",
    "\n",
    "        self._switch_status(self._GETHORING)\n",
    "        self._check_in()\n",
    "\n",
    "        self._switch_status(self._READY)\n",
    "        self.round = 1\n",
    "\n",
    "        for _ in range(self.rounds):\n",
    "            self._switch_status(self._IN_A_ROUND)\n",
    "            self._run_a_round()\n",
    "            self._switch_status(self._READY)\n",
    "            self.round += 1\n",
    "\n",
    "        if self.is_initiator:\n",
    "            self.push_log(f'Obtained the final results of task {self.task_id}')\n",
    "            self._switch_status(self._FINISHING)\n",
    "            self.test()\n",
    "            self._close_task()\n",
    "\n",
    "    def _check_in(self):\n",
    "        \"\"\"Check in task and get ready.\n",
    "\n",
    "        As an initiator (and default the aggregator), records each participants\n",
    "        and launches training process.\n",
    "        As a participant, checkins and gets ready for training.\n",
    "        \"\"\"\n",
    "        if self.is_initiator:\n",
    "            self.push_log('Waiting for participants taking part in ...')\n",
    "            self._wait_for_gathering()\n",
    "        else:\n",
    "            is_checked_in = False\n",
    "            # the aggregator may be in special state so can not response\n",
    "            # correctly nor in time, then retry periodically\n",
    "            self.push_log('Checking in the task ...')\n",
    "            while not is_checked_in:\n",
    "                is_checked_in = self._check_in_task()\n",
    "            self.push_log(f'Node {self.id} have taken part in the task.')\n",
    "\n",
    "    def _wait_for_gathering(self):\n",
    "        \"\"\"Wait for participants gethering.\"\"\"\n",
    "        logger.debug('_wait_for_gathering ...')\n",
    "        for _event in self.contractor.contract_events():\n",
    "            if isinstance(_event, CheckinEvent):\n",
    "                if _event.peer_id not in self._participants:\n",
    "                    self._participants.append(_event.peer_id)\n",
    "                    self.push_log(f'Welcome a new participant ID: {_event.peer_id}.')\n",
    "                    self.push_log(f'There are {len(self._participants)} participants now.')\n",
    "                self.contractor.respond_check_in(round=self.round,\n",
    "                                                 aggregator=self.id,\n",
    "                                                 nonce=_event.nonce,\n",
    "                                                 requester_id=_event.peer_id)\n",
    "                if len(self._participants) == self.clients:\n",
    "                    break\n",
    "        self.push_log('All participants gethered.')\n",
    "\n",
    "    def _check_in_task(self) -> bool:\n",
    "        \"\"\"Try to check in the task.\"\"\"\n",
    "        nonce = self.contractor.checkin(peer_id=self.id)\n",
    "        logger.debug('_wait_for_check_in_response ...')\n",
    "        for _event in self.contractor.contract_events(timeout=30):\n",
    "            if isinstance(_event, CheckinResponseEvent):\n",
    "                if _event.nonce != nonce:\n",
    "                    continue\n",
    "                self.round = _event.round\n",
    "                self._aggregator = _event.aggregator\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def _run_a_round(self):\n",
    "        \"\"\"Perform a round of FedAvg calculation.\n",
    "\n",
    "        As an aggregator, selects a part of participants as actual calculators\n",
    "        in the round, distributes latest parameters to them, collects update and\n",
    "        makes aggregation.\n",
    "        As a participant, if is selected as a calculator, calculates and uploads\n",
    "        parameter update.\n",
    "        \"\"\"\n",
    "        if self.is_initiator:\n",
    "            self._run_as_aggregator()\n",
    "        else:\n",
    "            self._run_as_data_owner()\n",
    "\n",
    "    def _run_as_aggregator(self):\n",
    "        self._start_round()\n",
    "        self._distribute_model()\n",
    "        self._process_aggregation()\n",
    "        self._close_round()\n",
    "\n",
    "    def _start_round(self):\n",
    "        \"\"\"Prepare and start calculation of a round.\"\"\"\n",
    "        self.push_log(f'Begin the training of round {self.round}.')\n",
    "        self.contractor.start_round(round=self.round,\n",
    "                                    calculators=self._participants,\n",
    "                                    aggregator=self.id)\n",
    "        self.push_log(f'Calculation of round {self.round} is started.')\n",
    "\n",
    "    def _distribute_model(self):\n",
    "        buffer = io.BytesIO()\n",
    "        torch.save(self.state_dict(), buffer)\n",
    "        self.push_log('Distributing parameters ...')\n",
    "        accept_list = self.data_channel.send_stream(source=self.id,\n",
    "                                                    target=self._participants,\n",
    "                                                    data_stream=buffer.getvalue())\n",
    "        self.push_log(f'Successfully distributed parameters to: {accept_list}')\n",
    "        if len(self._participants) != len(accept_list):\n",
    "            reject_list = [_target for _target in self._participants\n",
    "                           if _target not in accept_list]\n",
    "            self.push_log(f'Failed to distribute parameters to: {reject_list}')\n",
    "            raise RuntimeError('Failed to distribute parameters to some participants.')\n",
    "        self.push_log('Distributed parameters to all participants.')\n",
    "\n",
    "    def _process_aggregation(self):\n",
    "        \"\"\"Process aggregation depending on specific algorithm.\"\"\"\n",
    "        self._switch_status(self._WAIT_FOR_AGGR)\n",
    "        self.contractor.notify_ready_for_aggregation(round=self.round)\n",
    "        self.push_log('Now waiting for executing calculation ...')\n",
    "        accum_result, result_count = self._wait_for_calculation()\n",
    "        if result_count < self.clients:\n",
    "            self.push_log('Task failed because some calculation results lost.')\n",
    "            raise RuntimeError('Task failed because some calculation results lost.')\n",
    "        self.push_log(f'Received {result_count} copies of calculation results.')\n",
    "\n",
    "        self._switch_status(self._AGGREGATING)\n",
    "        self.push_log('Begin to aggregate and update parameters.')\n",
    "        for _key in accum_result.keys():\n",
    "            if accum_result[_key].dtype in (\n",
    "                torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64\n",
    "            ):\n",
    "                logger.warn(f'average a int value may lose precision: {_key=}')\n",
    "                accum_result[_key].div_(result_count, rounding_mode='trunc')\n",
    "            else:\n",
    "                accum_result[_key].div_(result_count)\n",
    "        self.load_state_dict(accum_result)\n",
    "        self.push_log('Obtained a new version of parameters.')\n",
    "\n",
    "    def _wait_for_calculation(self) -> Tuple[Dict[str, torch.Tensor], int]:\n",
    "        \"\"\"Wait for every calculator finish its task or timeout.\"\"\"\n",
    "        result_count = 0\n",
    "        accum_result = self.state_dict()\n",
    "        for _param in accum_result.values():\n",
    "            if isinstance(_param, torch.Tensor):\n",
    "                _param.zero_()\n",
    "\n",
    "        self.push_log('Waiting for training results ...')\n",
    "        while result_count < len(self._participants):\n",
    "            source, training_result = self.data_channel.receive_stream(receiver=self.id)\n",
    "            buffer = io.BytesIO(training_result)\n",
    "            _new_state_dict = torch.load(buffer)\n",
    "            for _key in accum_result.keys():\n",
    "                accum_result[_key].add_(_new_state_dict[_key])\n",
    "            result_count += 1\n",
    "            self.push_log(f'Received calculation results from ID: {source}')\n",
    "        return accum_result, result_count\n",
    "\n",
    "    def _close_round(self):\n",
    "        \"\"\"Close current round when finished.\"\"\"\n",
    "        self._switch_status(self._CLOSING_ROUND)\n",
    "        self.contractor.close_round(round=self.round)\n",
    "        self.push_log(f'The training of Round {self.round} complete.')\n",
    "\n",
    "    def _run_as_data_owner(self):\n",
    "        self._wait_for_starting_round()\n",
    "        self._switch_status(self._UPDATING)\n",
    "        self._wait_for_updating_model()\n",
    "\n",
    "        self._switch_status(self._CALCULATING)\n",
    "        self.push_log('Begin to run calculation ...')\n",
    "        self.train_an_epoch()\n",
    "        self.push_log('Local calculation complete.')\n",
    "\n",
    "        self._wait_for_uploading_model()\n",
    "        buffer = io.BytesIO()\n",
    "        torch.save(self.state_dict(), buffer)\n",
    "        self.push_log('Pushing local update to the aggregator ...')\n",
    "        self.data_channel.send_stream(source=self.id,\n",
    "                                      target=[self._aggregator],\n",
    "                                      data_stream=buffer.getvalue())\n",
    "        self.push_log('Successfully pushed local update to the aggregator.')\n",
    "        self._switch_status(self._CLOSING_ROUND)\n",
    "        self._wait_for_closing_round()\n",
    "\n",
    "        self.push_log(f'ID: {self.id} finished training task of round {self.round}.')\n",
    "\n",
    "    def _wait_for_starting_round(self):\n",
    "        \"\"\"Wait for starting a new round of training.\"\"\"\n",
    "        self.push_log(f'Waiting for training of round {self.round} begin ...')\n",
    "        for _event in self.contractor.contract_events():\n",
    "            if isinstance(_event, StartRoundEvent):\n",
    "                self.push_log(f'Training of round {self.round} begins.')\n",
    "                return\n",
    "\n",
    "    def _wait_for_updating_model(self):\n",
    "        \"\"\"Wait for receiving latest parameters from aggregator.\"\"\"\n",
    "        self.push_log('Waiting for receiving latest parameters from the aggregator ...')\n",
    "        _, parameters = self.data_channel.receive_stream(receiver=self.id)\n",
    "        buffer = io.BytesIO(parameters)\n",
    "        new_state_dict = torch.load(buffer)\n",
    "        self.load_state_dict(new_state_dict)\n",
    "        self.push_log('Successfully received latest parameters.')\n",
    "        return\n",
    "\n",
    "    def _wait_for_uploading_model(self):\n",
    "        \"\"\"Wait for uploading trained parameters to aggregator.\"\"\"\n",
    "        self.push_log('Waiting for aggregation begin ...')\n",
    "        for _event in self.contractor.contract_events():\n",
    "            if isinstance(_event, ReadyForAggregationEvent):\n",
    "                return\n",
    "\n",
    "    def _wait_for_closing_round(self):\n",
    "        \"\"\"Wait for closing current round of training.\"\"\"\n",
    "        self.push_log(f'Waiting for closing signal of training round {self.round} ...')\n",
    "        for _event in self.contractor.contract_events():\n",
    "            if isinstance(_event, CloseRoundEvent):\n",
    "                return\n",
    "\n",
    "    def _close_task(self, is_succ: bool = True):\n",
    "        \"\"\"Close the FedAvg calculation.\n",
    "\n",
    "        As an aggregator, broadcasts the finish task event to all participants,\n",
    "        uploads the final parameters and tells L1 task manager the task is complete.\n",
    "        As a participant, do nothing.\n",
    "        \"\"\"\n",
    "        self.push_log(f'Closing task {self.task_id} ...')\n",
    "        if self.is_initiator:\n",
    "            self._switch_status(self._FINISHING)\n",
    "            report_file_path, model_file_path = self._prepare_task_output()\n",
    "            self.contractor.upload_metric_report(receivers=self.contractor.EVERYONE,\n",
    "                                                 report_file=report_file_path)\n",
    "            self.contractor.upload_model(receivers=self.contractor.EVERYONE,\n",
    "                                         model_file=model_file_path)\n",
    "            self.contractor.notify_task_completion(result=True)\n",
    "        self.push_log(f'Task {self.task_id} closed. Byebye!')\n",
    "\n",
    "    def _prepare_task_output(self) -> Tuple[str, str]:\n",
    "        \"\"\"Generate final output files of the task.\n",
    "\n",
    "        Return:\n",
    "            Local paths of the report file and model file.\n",
    "        \"\"\"\n",
    "        self.push_log('Uploading task achievement and closing task ...')\n",
    "\n",
    "        os.makedirs(self._result_dir, exist_ok=True)\n",
    "\n",
    "        report_file = os.path.join(self._result_dir, \"report.zip\")\n",
    "        with ZipFile(report_file, 'w') as report_zip:\n",
    "            for path, _, filenames in os.walk(self._log_dir):\n",
    "                rel_dir = os.path.relpath(path=path, start=self._result_dir)\n",
    "                rel_dir = rel_dir.lstrip('.')  # ./file => file\n",
    "                for _file in filenames:\n",
    "                    rel_path = os.path.join(rel_dir, _file)\n",
    "                    report_zip.write(os.path.join(path, _file), rel_path)\n",
    "        report_file_path = os.path.abspath(report_file)\n",
    "\n",
    "        model_file = os.path.join(self._result_dir, \"model.pt\")\n",
    "        with open(model_file, 'wb') as f:\n",
    "            torch.save(self.state_dict(), f)\n",
    "        model_file_path = os.path.abspath(model_file)\n",
    "\n",
    "        self.push_log('Task achievement files are ready.')\n",
    "        return report_file_path, model_file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cf7f633b-58ca-4a6d-9b26-ee2041a23561",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import time\n",
    "from typing import Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from alphafed import get_dataset_dir, logger\n",
    "from torch.nn import Conv2d, Dropout2d, Linear, Module\n",
    "from torch.optim import SGD, Optimizer\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ba96eda1-7870-48ad-85fa-9ccf744c217d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv2d(in_channels=1, out_channels=10, kernel_size=5)\n",
    "        self.conv2 = Conv2d(in_channels=10, out_channels=20, kernel_size=5)\n",
    "        self.conv2_drop = Dropout2d()\n",
    "        self.fc1 = Linear(in_features=320, out_features=50)\n",
    "        self.fc2 = Linear(in_features=50, out_features=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8f11d3a3-4b5b-4842-8b5f-994eab859119",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTaskScheduler(SimpleFedAvgScheduler):\n",
    "\n",
    "    def __init__(self,\n",
    "                 clients: int,\n",
    "                 rounds: int,\n",
    "                 batch_size: int,\n",
    "                 learning_rate: float,\n",
    "                 momentum: float) -> None:\n",
    "        super().__init__(clients=clients, rounds=rounds)\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.seed = 42\n",
    "        torch.manual_seed(self.seed)\n",
    "\n",
    "    def build_model(self) -> Module:\n",
    "        model = ConvNet()\n",
    "        return model.to(self.device)\n",
    "\n",
    "    def build_optimizer(self, model: Module) -> Optimizer:\n",
    "        assert self.model, 'must initialize model first'\n",
    "        return SGD(self.model.parameters(),\n",
    "                   lr=self.learning_rate,\n",
    "                   momentum=self.momentum)\n",
    "\n",
    "    def build_train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            datasets.MNIST(\n",
    "                get_dataset_dir(self.task_id),\n",
    "                train=True,\n",
    "                download=True,\n",
    "                transform=transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.1307,), (0.3081,))\n",
    "                ])\n",
    "            ),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "    def build_test_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            datasets.MNIST(\n",
    "                get_dataset_dir(self.task_id),\n",
    "                train=False,\n",
    "                download=True,\n",
    "                transform=transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.1307,), (0.3081,))\n",
    "                ])\n",
    "            ),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "    def state_dict(self) -> Dict[str, torch.Tensor]:\n",
    "        return self.model.state_dict()\n",
    "\n",
    "    def load_state_dict(self, state_dict: Dict[str, torch.Tensor]):\n",
    "        self.model.load_state_dict(state_dict)\n",
    "\n",
    "    def train_an_epoch(self) -> None:\n",
    "        self.model.train()\n",
    "        for data, labels in self.train_loader:\n",
    "            data: torch.Tensor\n",
    "            labels: torch.Tensor\n",
    "            data, labels = data.to(self.device), labels.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(data)\n",
    "            loss = F.nll_loss(output, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "    def test(self):\n",
    "        start = time()\n",
    "        self.model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for data, labels in self.test_loader:\n",
    "                data, labels = data.to(self.device), labels.to(self.device)\n",
    "                output: torch.Tensor = self.model(data)\n",
    "                test_loss += F.nll_loss(output, labels, reduction='sum').item()\n",
    "                pred = output.max(1, keepdim=True)[1]\n",
    "                correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "\n",
    "        test_loss /= len(self.test_loader.dataset)\n",
    "        accuracy = correct / len(self.test_loader.dataset)\n",
    "        correct_rate = 100. * accuracy\n",
    "        logger.info(f'Test set: Average loss: {test_loss:.4f}')\n",
    "        logger.info(\n",
    "            f'Test set: Accuracy: {accuracy} ({correct_rate:.2f}%)'\n",
    "        )\n",
    "\n",
    "        end = time()\n",
    "\n",
    "        self.tb_writer.add_scalar('timer/run_time', end - start, self.round)\n",
    "        self.tb_writer.add_scalar('test_results/average_loss', test_loss, self.round)\n",
    "        self.tb_writer.add_scalar('test_results/accuracy', accuracy, self.round)\n",
    "        self.tb_writer.add_scalar('test_results/correct_rate', correct_rate, self.round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "01c5bc44-983e-4905-bfc3-e8f4cf640b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = SimpleTaskScheduler(clients=2,\n",
    "                                rounds=5,\n",
    "                                batch_size=128,\n",
    "                                learning_rate=0.01,\n",
    "                                momentum=0.9)\n",
    "scheduler.launch_task(task_id='943d472cc5a74d17a6a01d0e9a8f4707')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2c501f-96c8-4322-bc6d-cc2f6824091c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12 (default, Oct 12 2021, 06:23:56) \n[Clang 10.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
