{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d98d2bd6-abba-4429-94b2-a06dfeb74a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from cnn_net import ConvNet\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from alphafed.fed_avg import FedAvgScheduler\n",
    "\n",
    "\n",
    "class DemoAvg(FedAvgScheduler):\n",
    "\n",
    "    def __init__(self,\n",
    "                 max_rounds: int = 0,\n",
    "                 merge_epochs: int = 1,\n",
    "                 calculation_timeout: int = 300,\n",
    "                 log_rounds: int = 0,\n",
    "                 involve_aggregator: bool = False,\n",
    "                 batch_size: int = 128,\n",
    "                 learning_rate: float = 0.01,\n",
    "                 momentum: float = 0.9) -> None:\n",
    "        \"\"\"初始化参数说明.\n",
    "\n",
    "        以下为 `FedAvgScheduler` 父类定义的初始化参数。\n",
    "        max_rounds:\n",
    "            训练多少轮。\n",
    "        merge_epochs:\n",
    "            每次参数聚合前，在本地训练几个 epoch。\n",
    "        calculation_timeout:\n",
    "            本地训练超时时间。\n",
    "        log_rounds:\n",
    "            每隔几轮训练执行一次测试，评估记录当前训练效果。\n",
    "        involve_aggregator:\n",
    "            聚合方是否也使用本地数据参与训练，默认不参与，只负责聚合。\n",
    "\n",
    "        以下 `DemoAvg` 自定义的扩展初始化参数。\n",
    "        batch_size、learning_rate、momentum\n",
    "            训练参数。\n",
    "        \"\"\"\n",
    "        super().__init__(max_rounds=max_rounds,\n",
    "                         merge_epochs=merge_epochs,\n",
    "                         calculation_timeout=calculation_timeout,\n",
    "                         log_rounds=log_rounds,\n",
    "                         involve_aggregator=involve_aggregator)\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.seed = 42\n",
    "        torch.manual_seed(self.seed)\n",
    "\n",
    "    def build_model(self) -> nn.Module:  # 返回模型实例\n",
    "        model = ConvNet()\n",
    "        return model.to(self.device)\n",
    "\n",
    "    def build_optimizer(self, model: nn.Module) -> optim.Optimizer:  # 返回优化器实例\n",
    "        return optim.SGD(model.parameters(),\n",
    "                         lr=self.learning_rate,\n",
    "                         momentum=self.momentum)\n",
    "\n",
    "    # 网络下载 MNIST 训练数据用于训练，如果是实际使用场景，需要开发者自行处理数据加载\n",
    "    def build_train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            torchvision.datasets.MNIST(\n",
    "                'data',  # 数据下载目录\n",
    "                train=True,\n",
    "                download=True,\n",
    "                transform=torchvision.transforms.Compose([\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                    torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "                ])\n",
    "            ),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "    # 本示例不使用验证集数据，所以不需要实现 build_validation_dataloader\n",
    "\n",
    "    # 网络下载 MNIST 测试数据用于测试，如果是实际使用场景，需要开发者自行处理数据加载\n",
    "    def build_test_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            torchvision.datasets.MNIST(\n",
    "                'data',  # 数据下载目录\n",
    "                train=False,\n",
    "                download=True,\n",
    "                transform=torchvision.transforms.Compose([\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                    torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "                ])\n",
    "            ),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "    def train_an_epoch(self) -> None:  # 执行一个 epoch 训练的逻辑，与本地训练模型时的代码相同\n",
    "        self.model.train()\n",
    "        for data, labels in self.train_loader:\n",
    "            data, labels = data.to(self.device), labels.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(data)\n",
    "            loss = F.nll_loss(output, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "    def run_test(self):  # 执行一次测试的逻辑，与本地模型测时的代码相同\n",
    "        self.model.eval()\n",
    "        start = time()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for data, labels in self.test_loader:\n",
    "                data, labels = data.to(self.device), labels.to(self.device)\n",
    "                output: torch.Tensor = self.model(data)\n",
    "                test_loss += F.nll_loss(output, labels, reduction='sum').item()\n",
    "                pred = output.max(1, keepdim=True)[1]\n",
    "                correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "\n",
    "        end = time()\n",
    "\n",
    "        test_loss /= len(self.test_loader.dataset)\n",
    "        accuracy = correct / len(self.test_loader.dataset)\n",
    "        correct_rate = 100. * accuracy\n",
    "\n",
    "        # 记录 TensorBoard 日志，self.current_round 的值为“当前是训练的第几轮”\n",
    "        self.tb_writer.add_scalar('timer/run_time', end - start, self.current_round)\n",
    "        self.tb_writer.add_scalar('test_results/average_loss', test_loss, self.current_round)\n",
    "        self.tb_writer.add_scalar('test_results/accuracy', accuracy, self.current_round)\n",
    "        self.tb_writer.add_scalar('test_results/correct_rate', correct_rate, self.current_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6647171-687b-494f-a97a-5b90fd46c024",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-03 08:34:38,456|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='init'\n",
      "2023-02-03 08:34:38,480|INFO|scheduler|push_log|118:\n",
      "Begin to validate local context.\n",
      "2023-02-03 08:34:38,480|INFO|scheduler|push_log|118:\n",
      "Local context is ready.\n",
      "2023-02-03 08:34:38,482|INFO|scheduler|push_log|118:\n",
      "Node 1bb9feba-7b53-455b-b127-0eb19ffc9d3f is up.\n",
      "2023-02-03 08:34:38,482|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='gethering'\n",
      "2023-02-03 08:34:38,483|INFO|scheduler|push_log|118:\n",
      "Waiting for participants taking part in ...\n",
      "2023-02-03 08:34:38,483|DEBUG|fed_avg|_wait_for_gathering|432:\n",
      "_wait_for_gathering ...\n",
      "2023-02-03 08:35:02,523|INFO|scheduler|push_log|118:\n",
      "Welcome a new participant ID: 663ad4b0-b617-409f-8bc9-3682b30f7f30.\n",
      "2023-02-03 08:35:02,523|INFO|scheduler|push_log|118:\n",
      "There are 2 participants now.\n",
      "2023-02-03 08:35:17,572|INFO|scheduler|push_log|118:\n",
      "Welcome a new participant ID: 0fc1a571-2920-47bf-9e4e-b4edb7fa2caa.\n",
      "2023-02-03 08:35:17,573|INFO|scheduler|push_log|118:\n",
      "There are 3 participants now.\n",
      "2023-02-03 08:35:17,581|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='ready'\n",
      "2023-02-03 08:35:17,581|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='synchronizing'\n",
      "2023-02-03 08:35:17,582|INFO|scheduler|push_log|118:\n",
      "Synchronizing round state ...\n",
      "2023-02-03 08:35:17,584|INFO|scheduler|push_log|118:\n",
      "Initiate state synchronization of round 1.\n",
      "2023-02-03 08:35:17,601|INFO|scheduler|push_log|118:\n",
      "Waiting for synchronization responses ...\n",
      "2023-02-03 08:35:18,626|INFO|scheduler|push_log|118:\n",
      "Successfully synchronized state with ID: 0fc1a571-2920-47bf-9e4e-b4edb7fa2caa.\n",
      "2023-02-03 08:35:18,627|INFO|scheduler|push_log|118:\n",
      "Successfully synchronized with 1 participants.\n",
      "2023-02-03 08:35:18,640|INFO|scheduler|push_log|118:\n",
      "Successfully synchronized state with ID: 663ad4b0-b617-409f-8bc9-3682b30f7f30.\n",
      "2023-02-03 08:35:18,641|INFO|scheduler|push_log|118:\n",
      "Successfully synchronized with 2 participants.\n",
      "2023-02-03 08:35:18,642|INFO|scheduler|push_log|118:\n",
      "Successfully synchronized state in round 1\n",
      "2023-02-03 08:35:18,642|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='in_a_round'\n",
      "2023-02-03 08:35:18,643|INFO|scheduler|push_log|118:\n",
      "Begin the training of round 1.\n",
      "2023-02-03 08:35:18,652|INFO|scheduler|push_log|118:\n",
      "Calculation of round 1 is started.\n",
      "2023-02-03 08:35:18,654|INFO|scheduler|push_log|118:\n",
      "Distributing parameters ...\n",
      "2023-02-03 08:35:20,276|DEBUG|shared_file_data_channel|_do_send_stream|136:\n",
      "Sending data stream complete.\n",
      "2023-02-03 08:35:20,359|INFO|scheduler|push_log|118:\n",
      "Successfully distributed parameters to: ['0fc1a571-2920-47bf-9e4e-b4edb7fa2caa', '663ad4b0-b617-409f-8bc9-3682b30f7f30']\n",
      "2023-02-03 08:35:20,360|INFO|scheduler|push_log|118:\n",
      "Distributed parameters to 2 calculators.\n",
      "2023-02-03 08:35:20,361|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='wait_4_aggr'\n",
      "2023-02-03 08:35:20,370|INFO|scheduler|push_log|118:\n",
      "Now waiting for executing calculation ...\n",
      "2023-02-03 08:35:20,371|INFO|scheduler|push_log|118:\n",
      "Waiting for training results ...\n",
      "2023-02-03 08:35:38,266|INFO|scheduler|push_log|118:\n",
      "Received calculation results from ID: 0fc1a571-2920-47bf-9e4e-b4edb7fa2caa\n",
      "2023-02-03 08:35:38,268|INFO|scheduler|push_log|118:\n",
      "Received calculation results from ID: 663ad4b0-b617-409f-8bc9-3682b30f7f30\n",
      "2023-02-03 08:35:38,268|INFO|scheduler|push_log|118:\n",
      "Received 2 copies of calculation results.\n",
      "2023-02-03 08:35:38,269|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='aggregating'\n",
      "2023-02-03 08:35:38,269|INFO|scheduler|push_log|118:\n",
      "Begin to aggregate and update parameters.\n",
      "2023-02-03 08:35:38,271|INFO|scheduler|push_log|118:\n",
      "Obtained a new version of parameters.\n",
      "2023-02-03 08:35:38,271|INFO|scheduler|push_log|118:\n",
      "Begin to make a model test.\n",
      "2023-02-03 08:35:39,728|INFO|scheduler|push_log|118:\n",
      "Finished a round of test.\n",
      "2023-02-03 08:35:39,730|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='closing_round'\n",
      "2023-02-03 08:35:39,744|INFO|scheduler|push_log|118:\n",
      "The training of Round 1 complete.\n",
      "2023-02-03 08:35:39,744|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='ready'\n",
      "2023-02-03 08:35:39,745|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='persisting'\n",
      "2023-02-03 08:35:39,747|INFO|scheduler|push_log|118:\n",
      "Saved latest parameters locally.\n",
      "2023-02-03 08:35:39,748|INFO|scheduler|push_log|118:\n",
      "Saved latest runtime context.\n",
      "2023-02-03 08:35:39,749|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='ready'\n",
      "2023-02-03 08:35:39,749|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='synchronizing'\n",
      "2023-02-03 08:35:39,750|INFO|scheduler|push_log|118:\n",
      "Synchronizing round state ...\n",
      "2023-02-03 08:35:39,750|INFO|scheduler|push_log|118:\n",
      "Initiate state synchronization of round 2.\n",
      "2023-02-03 08:35:39,777|INFO|scheduler|push_log|118:\n",
      "Waiting for synchronization responses ...\n",
      "2023-02-03 08:35:40,798|INFO|scheduler|push_log|118:\n",
      "Successfully synchronized state with ID: 663ad4b0-b617-409f-8bc9-3682b30f7f30.\n",
      "2023-02-03 08:35:40,799|INFO|scheduler|push_log|118:\n",
      "Successfully synchronized with 1 participants.\n",
      "2023-02-03 08:35:40,809|INFO|scheduler|push_log|118:\n",
      "Successfully synchronized state with ID: 0fc1a571-2920-47bf-9e4e-b4edb7fa2caa.\n",
      "2023-02-03 08:35:40,810|INFO|scheduler|push_log|118:\n",
      "Successfully synchronized with 2 participants.\n",
      "2023-02-03 08:35:40,810|INFO|scheduler|push_log|118:\n",
      "Successfully synchronized state in round 2\n",
      "2023-02-03 08:35:40,811|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='in_a_round'\n",
      "2023-02-03 08:35:40,811|INFO|scheduler|push_log|118:\n",
      "Begin the training of round 2.\n",
      "2023-02-03 08:35:40,839|INFO|scheduler|push_log|118:\n",
      "Calculation of round 2 is started.\n",
      "2023-02-03 08:35:40,841|INFO|scheduler|push_log|118:\n",
      "Distributing parameters ...\n",
      "2023-02-03 08:35:42,005|DEBUG|shared_file_data_channel|_do_send_stream|136:\n",
      "Sending data stream complete.\n",
      "2023-02-03 08:35:42,044|INFO|scheduler|push_log|118:\n",
      "Successfully distributed parameters to: ['663ad4b0-b617-409f-8bc9-3682b30f7f30', '0fc1a571-2920-47bf-9e4e-b4edb7fa2caa']\n",
      "2023-02-03 08:35:42,045|INFO|scheduler|push_log|118:\n",
      "Distributed parameters to 2 calculators.\n",
      "2023-02-03 08:35:42,047|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='wait_4_aggr'\n",
      "2023-02-03 08:35:42,055|INFO|scheduler|push_log|118:\n",
      "Now waiting for executing calculation ...\n",
      "2023-02-03 08:35:42,056|INFO|scheduler|push_log|118:\n",
      "Waiting for training results ...\n",
      "2023-02-03 08:35:58,536|INFO|scheduler|push_log|118:\n",
      "Received calculation results from ID: 663ad4b0-b617-409f-8bc9-3682b30f7f30\n",
      "2023-02-03 08:35:58,539|INFO|scheduler|push_log|118:\n",
      "Received calculation results from ID: 0fc1a571-2920-47bf-9e4e-b4edb7fa2caa\n",
      "2023-02-03 08:35:58,540|INFO|scheduler|push_log|118:\n",
      "Received 2 copies of calculation results.\n",
      "2023-02-03 08:35:58,541|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='aggregating'\n",
      "2023-02-03 08:35:58,541|INFO|scheduler|push_log|118:\n",
      "Begin to aggregate and update parameters.\n",
      "2023-02-03 08:35:58,543|INFO|scheduler|push_log|118:\n",
      "Obtained a new version of parameters.\n",
      "2023-02-03 08:35:58,544|INFO|scheduler|push_log|118:\n",
      "Begin to make a model test.\n",
      "2023-02-03 08:35:59,959|INFO|scheduler|push_log|118:\n",
      "Finished a round of test.\n",
      "2023-02-03 08:35:59,960|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='closing_round'\n",
      "2023-02-03 08:35:59,969|INFO|scheduler|push_log|118:\n",
      "The training of Round 2 complete.\n",
      "2023-02-03 08:35:59,969|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='ready'\n",
      "2023-02-03 08:35:59,970|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='persisting'\n",
      "2023-02-03 08:35:59,972|INFO|scheduler|push_log|118:\n",
      "Saved latest parameters locally.\n",
      "2023-02-03 08:35:59,973|INFO|scheduler|push_log|118:\n",
      "Saved latest runtime context.\n",
      "2023-02-03 08:35:59,973|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='ready'\n",
      "2023-02-03 08:35:59,974|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='synchronizing'\n",
      "2023-02-03 08:35:59,975|INFO|scheduler|push_log|118:\n",
      "Synchronizing round state ...\n",
      "2023-02-03 08:35:59,975|INFO|scheduler|push_log|118:\n",
      "Initiate state synchronization of round 3.\n",
      "2023-02-03 08:35:59,987|INFO|scheduler|push_log|118:\n",
      "Waiting for synchronization responses ...\n",
      "2023-02-03 08:36:01,038|INFO|scheduler|push_log|118:\n",
      "Successfully synchronized state with ID: 663ad4b0-b617-409f-8bc9-3682b30f7f30.\n",
      "2023-02-03 08:36:01,039|INFO|scheduler|push_log|118:\n",
      "Successfully synchronized with 1 participants.\n",
      "2023-02-03 08:36:01,046|INFO|scheduler|push_log|118:\n",
      "Successfully synchronized state with ID: 0fc1a571-2920-47bf-9e4e-b4edb7fa2caa.\n",
      "2023-02-03 08:36:01,047|INFO|scheduler|push_log|118:\n",
      "Successfully synchronized with 2 participants.\n",
      "2023-02-03 08:36:01,047|INFO|scheduler|push_log|118:\n",
      "Successfully synchronized state in round 3\n",
      "2023-02-03 08:36:01,048|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='in_a_round'\n",
      "2023-02-03 08:36:01,049|INFO|scheduler|push_log|118:\n",
      "Begin the training of round 3.\n",
      "2023-02-03 08:36:01,073|INFO|scheduler|push_log|118:\n",
      "Calculation of round 3 is started.\n",
      "2023-02-03 08:36:01,075|INFO|scheduler|push_log|118:\n",
      "Distributing parameters ...\n",
      "2023-02-03 08:36:02,269|DEBUG|shared_file_data_channel|_do_send_stream|136:\n",
      "Sending data stream complete.\n",
      "2023-02-03 08:36:02,282|INFO|scheduler|push_log|118:\n",
      "Successfully distributed parameters to: ['663ad4b0-b617-409f-8bc9-3682b30f7f30', '0fc1a571-2920-47bf-9e4e-b4edb7fa2caa']\n",
      "2023-02-03 08:36:02,283|INFO|scheduler|push_log|118:\n",
      "Distributed parameters to 2 calculators.\n",
      "2023-02-03 08:36:02,283|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='wait_4_aggr'\n",
      "2023-02-03 08:36:02,298|INFO|scheduler|push_log|118:\n",
      "Now waiting for executing calculation ...\n",
      "2023-02-03 08:36:02,299|INFO|scheduler|push_log|118:\n",
      "Waiting for training results ...\n",
      "2023-02-03 08:36:19,071|INFO|scheduler|push_log|118:\n",
      "Received calculation results from ID: 0fc1a571-2920-47bf-9e4e-b4edb7fa2caa\n",
      "2023-02-03 08:36:19,073|INFO|scheduler|push_log|118:\n",
      "Received calculation results from ID: 663ad4b0-b617-409f-8bc9-3682b30f7f30\n",
      "2023-02-03 08:36:19,074|INFO|scheduler|push_log|118:\n",
      "Received 2 copies of calculation results.\n",
      "2023-02-03 08:36:19,074|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='aggregating'\n",
      "2023-02-03 08:36:19,075|INFO|scheduler|push_log|118:\n",
      "Begin to aggregate and update parameters.\n",
      "2023-02-03 08:36:19,076|INFO|scheduler|push_log|118:\n",
      "Obtained a new version of parameters.\n",
      "2023-02-03 08:36:19,077|INFO|scheduler|push_log|118:\n",
      "Begin to make a model test.\n",
      "2023-02-03 08:36:20,491|INFO|scheduler|push_log|118:\n",
      "Finished a round of test.\n",
      "2023-02-03 08:36:20,494|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='closing_round'\n",
      "2023-02-03 08:36:20,502|INFO|scheduler|push_log|118:\n",
      "The training of Round 3 complete.\n",
      "2023-02-03 08:36:20,502|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='ready'\n",
      "2023-02-03 08:36:20,503|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='persisting'\n",
      "2023-02-03 08:36:20,505|INFO|scheduler|push_log|118:\n",
      "Saved latest parameters locally.\n",
      "2023-02-03 08:36:20,506|INFO|scheduler|push_log|118:\n",
      "Saved latest runtime context.\n",
      "2023-02-03 08:36:20,507|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='ready'\n",
      "2023-02-03 08:36:20,507|INFO|scheduler|push_log|118:\n",
      "Obtained the final results of task cbb3ffd0-838c-41ca-a41a-7c11cae29181\n",
      "2023-02-03 08:36:20,508|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='finishing'\n",
      "2023-02-03 08:36:20,509|INFO|scheduler|push_log|118:\n",
      "Closing task cbb3ffd0-838c-41ca-a41a-7c11cae29181 ...\n",
      "2023-02-03 08:36:20,509|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='finishing'\n",
      "2023-02-03 08:36:20,526|INFO|scheduler|push_log|118:\n",
      "Uploading task achievement and closing task ...\n",
      "2023-02-03 08:36:20,528|INFO|scheduler|push_log|118:\n",
      "Task achievement files are ready.\n",
      "2023-02-03 08:36:20,740|INFO|scheduler|push_log|118:\n",
      "Task cbb3ffd0-838c-41ca-a41a-7c11cae29181 closed. Byebye!\n"
     ]
    }
   ],
   "source": [
    "from alphafed import mock_context\n",
    "\n",
    "# 聚合方的模拟启动脚本\n",
    "scheduler = DemoAvg(\n",
    "    max_rounds=3,  # 受本地资源限制，运行速度可能会很慢，调试时不建议设置太高\n",
    "    log_rounds=1,\n",
    "    calculation_timeout=1800  # 受本地资源限制，运行速度可能会很慢，设置太低容易导致超时\n",
    ")\n",
    "\n",
    "task_id = 'cbb3ffd0-838c-41ca-a41a-7c11cae29181'  # 设置一个假想 ID\n",
    "# 算法实际运行时会从任务管理器获取任务参与节点的 Node ID 列表，但是在模拟环境不能通过\n",
    "# 访问实际接口获得这个信息，所以需要通过 nodes 参数将这个列表配置在模拟环境中。\n",
    "aggregator_id = '1bb9feba-7b53-455b-b127-0eb19ffc9d3f'  # 设置一个假想 ID\n",
    "col_id_1 = '663ad4b0-b617-409f-8bc9-3682b30f7f30'  # 设置一个假想 ID\n",
    "col_id_2 = '0fc1a571-2920-47bf-9e4e-b4edb7fa2caa'  # 设置一个假想 ID\n",
    "with mock_context(id=aggregator_id, nodes=[aggregator_id, col_id_1, col_id_2]):  # 在模拟调试环境中运行\n",
    "    scheduler._run(id=aggregator_id, task_id=task_id, is_initiator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015da686-df19-48a7-a895-ec3ff3b372aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "1f0c7602c82e39efa19a01e5e068584db7a6d17aff8711ab06660aac81377393"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
