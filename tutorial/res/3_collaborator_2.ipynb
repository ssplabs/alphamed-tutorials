{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bec8938-59fe-431c-9ee4-358881689b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from cnn_net import ConvNet\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from alphafed.fed_avg import FedAvgScheduler\n",
    "\n",
    "\n",
    "class DemoAvg(FedAvgScheduler):\n",
    "\n",
    "    def __init__(self,\n",
    "                 max_rounds: int = 0,\n",
    "                 merge_epochs: int = 1,\n",
    "                 calculation_timeout: int = 300,\n",
    "                 log_rounds: int = 0,\n",
    "                 involve_aggregator: bool = False,\n",
    "                 batch_size: int = 128,\n",
    "                 learning_rate: float = 0.01,\n",
    "                 momentum: float = 0.9) -> None:\n",
    "        \"\"\"初始化参数说明.\n",
    "\n",
    "        以下为 `FedAvgScheduler` 父类定义的初始化参数。\n",
    "        max_rounds:\n",
    "            训练多少轮。\n",
    "        merge_epochs:\n",
    "            每次参数聚合前，在本地训练几个 epoch。\n",
    "        calculation_timeout:\n",
    "            本地训练超时时间。\n",
    "        log_rounds:\n",
    "            每隔几轮训练执行一次测试，评估记录当前训练效果。\n",
    "        involve_aggregator:\n",
    "            聚合方是否也使用本地数据参与训练，默认不参与，只负责聚合。\n",
    "\n",
    "        以下 `DemoAvg` 自定义的扩展初始化参数。\n",
    "        batch_size、learning_rate、momentum\n",
    "            训练参数。\n",
    "        \"\"\"\n",
    "        super().__init__(max_rounds=max_rounds,\n",
    "                         merge_epochs=merge_epochs,\n",
    "                         calculation_timeout=calculation_timeout,\n",
    "                         log_rounds=log_rounds,\n",
    "                         involve_aggregator=involve_aggregator)\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.seed = 42\n",
    "        torch.manual_seed(self.seed)\n",
    "\n",
    "    def build_model(self) -> nn.Module:  # 返回模型实例\n",
    "        model = ConvNet()\n",
    "        return model.to(self.device)\n",
    "\n",
    "    def build_optimizer(self, model: nn.Module) -> optim.Optimizer:  # 返回优化器实例\n",
    "        return optim.SGD(model.parameters(),\n",
    "                         lr=self.learning_rate,\n",
    "                         momentum=self.momentum)\n",
    "\n",
    "    # 网络下载 MNIST 训练数据用于训练，如果是实际使用场景，需要开发者自行处理数据加载\n",
    "    def build_train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            torchvision.datasets.MNIST(\n",
    "                'data',  # 数据下载目录\n",
    "                train=True,\n",
    "                download=True,\n",
    "                transform=torchvision.transforms.Compose([\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                    torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "                ])\n",
    "            ),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "    # 本示例不使用验证集数据，所以不需要实现 build_validation_dataloader\n",
    "\n",
    "    # 网络下载 MNIST 测试数据用于测试，如果是实际使用场景，需要开发者自行处理数据加载\n",
    "    def build_test_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            torchvision.datasets.MNIST(\n",
    "                'data',  # 数据下载目录\n",
    "                train=False,\n",
    "                download=True,\n",
    "                transform=torchvision.transforms.Compose([\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                    torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "                ])\n",
    "            ),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "    def train_an_epoch(self) -> None:  # 执行一个 epoch 训练的逻辑，与本地训练模型时的代码相同\n",
    "        self.model.train()\n",
    "        for data, labels in self.train_loader:\n",
    "            data, labels = data.to(self.device), labels.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(data)\n",
    "            loss = F.nll_loss(output, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "    def run_test(self):  # 执行一次测试的逻辑，与本地模型测时的代码相同\n",
    "        self.model.eval()\n",
    "        start = time()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for data, labels in self.test_loader:\n",
    "                data, labels = data.to(self.device), labels.to(self.device)\n",
    "                output: torch.Tensor = self.model(data)\n",
    "                test_loss += F.nll_loss(output, labels, reduction='sum').item()\n",
    "                pred = output.max(1, keepdim=True)[1]\n",
    "                correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "\n",
    "        end = time()\n",
    "\n",
    "        test_loss /= len(self.test_loader.dataset)\n",
    "        accuracy = correct / len(self.test_loader.dataset)\n",
    "        correct_rate = 100. * accuracy\n",
    "\n",
    "        # 记录 TensorBoard 日志，self.current_round 的值为“当前是训练的第几轮”\n",
    "        self.tb_writer.add_scalar('timer/run_time', end - start, self.current_round)\n",
    "        self.tb_writer.add_scalar('test_results/average_loss', test_loss, self.current_round)\n",
    "        self.tb_writer.add_scalar('test_results/accuracy', accuracy, self.current_round)\n",
    "        self.tb_writer.add_scalar('test_results/correct_rate', correct_rate, self.current_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "574c5b6d-7256-4424-b7ba-8ec02c816ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-03 08:35:16,878|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='init'\n",
      "2023-02-03 08:35:16,896|INFO|scheduler|push_log|118:\n",
      "Begin to validate local context.\n",
      "2023-02-03 08:35:16,897|INFO|scheduler|push_log|118:\n",
      "Local context is ready.\n",
      "2023-02-03 08:35:16,898|INFO|scheduler|push_log|118:\n",
      "Node 0fc1a571-2920-47bf-9e4e-b4edb7fa2caa is up.\n",
      "2023-02-03 08:35:16,898|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='gethering'\n",
      "2023-02-03 08:35:16,899|INFO|scheduler|push_log|118:\n",
      "Checking in the task ...\n",
      "2023-02-03 08:35:16,911|DEBUG|fed_avg|_wait_for_check_in_response|479:\n",
      "_wait_for_check_in_response ...\n",
      "2023-02-03 08:35:17,935|INFO|scheduler|push_log|118:\n",
      "Node 0fc1a571-2920-47bf-9e4e-b4edb7fa2caa have taken part in the task.\n",
      "2023-02-03 08:35:17,936|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='ready'\n",
      "2023-02-03 08:35:17,937|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='synchronizing'\n",
      "2023-02-03 08:35:17,937|INFO|scheduler|push_log|118:\n",
      "Synchronizing round state ...\n",
      "2023-02-03 08:35:17,938|INFO|scheduler|push_log|118:\n",
      "Waiting for synchronizing state with the aggregator ...\n",
      "2023-02-03 08:35:17,964|INFO|scheduler|push_log|118:\n",
      "Successfully synchronized state with the aggregator.\n",
      "2023-02-03 08:35:17,964|INFO|scheduler|push_log|118:\n",
      "Successfully synchronized state in round 1\n",
      "2023-02-03 08:35:17,965|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='in_a_round'\n",
      "2023-02-03 08:35:17,966|INFO|scheduler|push_log|118:\n",
      "Waiting for training of round 1 begin ...\n",
      "2023-02-03 08:35:18,974|INFO|scheduler|push_log|118:\n",
      "Training of round 1 begins.\n",
      "2023-02-03 08:35:18,975|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='updating'\n",
      "2023-02-03 08:35:18,975|INFO|scheduler|push_log|118:\n",
      "Waiting for receiving latest parameters from the aggregator ...\n",
      "2023-02-03 08:35:20,932|INFO|scheduler|push_log|118:\n",
      "Successfully received latest parameters.\n",
      "2023-02-03 08:35:20,934|INFO|scheduler|push_log|118:\n",
      "Saved latest parameters locally.\n",
      "2023-02-03 08:35:20,935|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='calculating'\n",
      "2023-02-03 08:35:20,936|INFO|scheduler|push_log|118:\n",
      "Begin to run calculation ...\n",
      "2023-02-03 08:35:36,454|INFO|scheduler|push_log|118:\n",
      "Local calculation complete.\n",
      "2023-02-03 08:35:36,458|INFO|scheduler|push_log|118:\n",
      "Waiting for aggregation begin ...\n",
      "2023-02-03 08:35:36,470|INFO|scheduler|push_log|118:\n",
      "Pushing local update to the aggregator ...\n",
      "2023-02-03 08:35:37,600|DEBUG|shared_file_data_channel|_do_send_stream|136:\n",
      "Sending data stream complete.\n",
      "2023-02-03 08:35:37,673|INFO|scheduler|push_log|118:\n",
      "Successfully pushed local update to the aggregator.\n",
      "2023-02-03 08:35:37,674|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='closing_round'\n",
      "2023-02-03 08:35:37,675|INFO|scheduler|push_log|118:\n",
      "Waiting for closing signal of training round 1 ...\n",
      "2023-02-03 08:35:40,703|INFO|scheduler|push_log|118:\n",
      "ID: 0fc1a571-2920-47bf-9e4e-b4edb7fa2caa finished training task of round 1.\n",
      "2023-02-03 08:35:40,704|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='ready'\n",
      "2023-02-03 08:35:40,705|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='synchronizing'\n",
      "2023-02-03 08:35:40,705|INFO|scheduler|push_log|118:\n",
      "Synchronizing round state ...\n",
      "2023-02-03 08:35:40,706|INFO|scheduler|push_log|118:\n",
      "Waiting for synchronizing state with the aggregator ...\n",
      "2023-02-03 08:35:40,724|INFO|scheduler|push_log|118:\n",
      "Successfully synchronized state with the aggregator.\n",
      "2023-02-03 08:35:40,724|INFO|scheduler|push_log|118:\n",
      "Successfully synchronized state in round 2\n",
      "2023-02-03 08:35:40,725|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='in_a_round'\n",
      "2023-02-03 08:35:40,725|INFO|scheduler|push_log|118:\n",
      "Waiting for training of round 2 begin ...\n",
      "2023-02-03 08:35:41,734|INFO|scheduler|push_log|118:\n",
      "Training of round 2 begins.\n",
      "2023-02-03 08:35:41,735|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='updating'\n",
      "2023-02-03 08:35:41,736|INFO|scheduler|push_log|118:\n",
      "Waiting for receiving latest parameters from the aggregator ...\n",
      "2023-02-03 08:35:42,372|INFO|scheduler|push_log|118:\n",
      "Successfully received latest parameters.\n",
      "2023-02-03 08:35:42,375|INFO|scheduler|push_log|118:\n",
      "Saved latest parameters locally.\n",
      "2023-02-03 08:35:42,376|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='calculating'\n",
      "2023-02-03 08:35:42,376|INFO|scheduler|push_log|118:\n",
      "Begin to run calculation ...\n",
      "2023-02-03 08:35:57,238|INFO|scheduler|push_log|118:\n",
      "Local calculation complete.\n",
      "2023-02-03 08:35:57,239|INFO|scheduler|push_log|118:\n",
      "Waiting for aggregation begin ...\n",
      "2023-02-03 08:35:57,247|INFO|scheduler|push_log|118:\n",
      "Pushing local update to the aggregator ...\n",
      "2023-02-03 08:35:58,393|DEBUG|shared_file_data_channel|_do_send_stream|136:\n",
      "Sending data stream complete.\n",
      "2023-02-03 08:35:58,450|INFO|scheduler|push_log|118:\n",
      "Successfully pushed local update to the aggregator.\n",
      "2023-02-03 08:35:58,450|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='closing_round'\n",
      "2023-02-03 08:35:58,451|INFO|scheduler|push_log|118:\n",
      "Waiting for closing signal of training round 2 ...\n",
      "2023-02-03 08:36:00,466|INFO|scheduler|push_log|118:\n",
      "ID: 0fc1a571-2920-47bf-9e4e-b4edb7fa2caa finished training task of round 2.\n",
      "2023-02-03 08:36:00,467|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='ready'\n",
      "2023-02-03 08:36:00,468|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='synchronizing'\n",
      "2023-02-03 08:36:00,468|INFO|scheduler|push_log|118:\n",
      "Synchronizing round state ...\n",
      "2023-02-03 08:36:00,469|INFO|scheduler|push_log|118:\n",
      "Waiting for synchronizing state with the aggregator ...\n",
      "2023-02-03 08:36:00,483|INFO|scheduler|push_log|118:\n",
      "Successfully synchronized state with the aggregator.\n",
      "2023-02-03 08:36:00,484|INFO|scheduler|push_log|118:\n",
      "Successfully synchronized state in round 3\n",
      "2023-02-03 08:36:00,484|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='in_a_round'\n",
      "2023-02-03 08:36:00,485|INFO|scheduler|push_log|118:\n",
      "Waiting for training of round 3 begin ...\n",
      "2023-02-03 08:36:01,503|INFO|scheduler|push_log|118:\n",
      "Training of round 3 begins.\n",
      "2023-02-03 08:36:01,504|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='updating'\n",
      "2023-02-03 08:36:01,505|INFO|scheduler|push_log|118:\n",
      "Waiting for receiving latest parameters from the aggregator ...\n",
      "2023-02-03 08:36:01,931|INFO|scheduler|push_log|118:\n",
      "Successfully received latest parameters.\n",
      "2023-02-03 08:36:01,934|INFO|scheduler|push_log|118:\n",
      "Saved latest parameters locally.\n",
      "2023-02-03 08:36:01,934|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='calculating'\n",
      "2023-02-03 08:36:01,935|INFO|scheduler|push_log|118:\n",
      "Begin to run calculation ...\n",
      "2023-02-03 08:36:17,825|INFO|scheduler|push_log|118:\n",
      "Local calculation complete.\n",
      "2023-02-03 08:36:17,826|INFO|scheduler|push_log|118:\n",
      "Waiting for aggregation begin ...\n",
      "2023-02-03 08:36:17,837|INFO|scheduler|push_log|118:\n",
      "Pushing local update to the aggregator ...\n",
      "2023-02-03 08:36:19,012|DEBUG|shared_file_data_channel|_do_send_stream|136:\n",
      "Sending data stream complete.\n",
      "2023-02-03 08:36:19,043|INFO|scheduler|push_log|118:\n",
      "Successfully pushed local update to the aggregator.\n",
      "2023-02-03 08:36:19,044|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='closing_round'\n",
      "2023-02-03 08:36:19,044|INFO|scheduler|push_log|118:\n",
      "Waiting for closing signal of training round 3 ...\n",
      "2023-02-03 08:36:21,063|INFO|scheduler|push_log|118:\n",
      "ID: 0fc1a571-2920-47bf-9e4e-b4edb7fa2caa finished training task of round 3.\n",
      "2023-02-03 08:36:21,064|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='ready'\n",
      "2023-02-03 08:36:21,065|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='synchronizing'\n",
      "2023-02-03 08:36:21,065|INFO|scheduler|push_log|118:\n",
      "Synchronizing round state ...\n",
      "2023-02-03 08:36:21,066|INFO|scheduler|push_log|118:\n",
      "Waiting for synchronizing state with the aggregator ...\n",
      "2023-02-03 08:36:21,073|INFO|fed_avg|_launch_process|352:\n",
      "training task complete\n"
     ]
    }
   ],
   "source": [
    "from alphafed import mock_context\n",
    "\n",
    "# 另一个参与方的模拟启动脚本，需要复制到另一个 Notebook 脚本文件中执行\n",
    "# scheduler 实例和聚合方的一模一样\n",
    "scheduler = DemoAvg(max_rounds=3, log_rounds=1, calculation_timeout=1800)\n",
    "\n",
    "task_id = 'cbb3ffd0-838c-41ca-a41a-7c11cae29181'  # 任务 ID 必须与聚合方一致\n",
    "aggregator_id = '1bb9feba-7b53-455b-b127-0eb19ffc9d3f'  # 必须与聚合方配置的一致\n",
    "col_id_1 = '663ad4b0-b617-409f-8bc9-3682b30f7f30'  # 必须与聚合方配置的一致\n",
    "col_id_2 = '0fc1a571-2920-47bf-9e4e-b4edb7fa2caa'  # 必须与聚合方配置的一致\n",
    "with mock_context(id=col_id_2, nodes=[aggregator_id, col_id_1, col_id_2]):  # 在模拟调试环境中运行\n",
    "    scheduler._run(id=col_id_2, task_id=task_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788086c6-c451-411e-81cb-90b3ebebf7a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "1f0c7602c82e39efa19a01e5e068584db7a6d17aff8711ab06660aac81377393"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
