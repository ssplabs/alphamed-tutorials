{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "977681af-127b-43d7-9200-5804284f56dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from cnn_net import ConvNet\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from alphafed.fed_avg import FedAvgScheduler\n",
    "\n",
    "\n",
    "class DemoAvg(FedAvgScheduler):\n",
    "\n",
    "    def __init__(self,\n",
    "                 max_rounds: int = 0,\n",
    "                 merge_epochs: int = 1,\n",
    "                 calculation_timeout: int = 300,\n",
    "                 log_rounds: int = 0,\n",
    "                 involve_aggregator: bool = False,\n",
    "                 batch_size: int = 128,\n",
    "                 learning_rate: float = 0.01,\n",
    "                 momentum: float = 0.9) -> None:\n",
    "        \"\"\"初始化参数说明.\n",
    "\n",
    "        以下为 `FedAvgScheduler` 父类定义的初始化参数。\n",
    "        max_rounds:\n",
    "            训练多少轮。\n",
    "        merge_epochs:\n",
    "            每次参数聚合前，在本地训练几个 epoch。\n",
    "        calculation_timeout:\n",
    "            本地训练超时时间。\n",
    "        log_rounds:\n",
    "            每隔几轮训练执行一次测试，评估记录当前训练效果。\n",
    "        involve_aggregator:\n",
    "            聚合方是否也使用本地数据参与训练，默认不参与，只负责聚合。\n",
    "\n",
    "        以下 `DemoAvg` 自定义的扩展初始化参数。\n",
    "        batch_size、learning_rate、momentum\n",
    "            训练参数。\n",
    "        \"\"\"\n",
    "        super().__init__(max_rounds=max_rounds,\n",
    "                         merge_epochs=merge_epochs,\n",
    "                         calculation_timeout=calculation_timeout,\n",
    "                         log_rounds=log_rounds,\n",
    "                         involve_aggregator=involve_aggregator)\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.seed = 42\n",
    "        torch.manual_seed(self.seed)\n",
    "\n",
    "    def build_model(self) -> nn.Module:  # 返回模型实例\n",
    "        model = ConvNet()\n",
    "        return model.to(self.device)\n",
    "\n",
    "    def build_optimizer(self, model: nn.Module) -> optim.Optimizer:  # 返回优化器实例\n",
    "        return optim.SGD(model.parameters(),\n",
    "                         lr=self.learning_rate,\n",
    "                         momentum=self.momentum)\n",
    "\n",
    "    # 网络下载 MNIST 训练数据用于训练，如果是实际使用场景，需要开发者自行处理数据加载\n",
    "    def build_train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            torchvision.datasets.MNIST(\n",
    "                'data',  # 数据下载目录\n",
    "                train=True,\n",
    "                download=True,\n",
    "                transform=torchvision.transforms.Compose([\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                    torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "                ])\n",
    "            ),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "    # 本示例不使用验证集数据，所以不需要实现 build_validation_dataloader\n",
    "\n",
    "    # 网络下载 MNIST 测试数据用于测试，如果是实际使用场景，需要开发者自行处理数据加载\n",
    "    def build_test_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            torchvision.datasets.MNIST(\n",
    "                'data',  # 数据下载目录\n",
    "                train=False,\n",
    "                download=True,\n",
    "                transform=torchvision.transforms.Compose([\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                    torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "                ])\n",
    "            ),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "    def train_an_epoch(self) -> None:  # 执行一个 epoch 训练的逻辑，与本地训练模型时的代码相同\n",
    "        self.model.train()\n",
    "        for data, labels in self.train_loader:\n",
    "            data, labels = data.to(self.device), labels.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(data)\n",
    "            loss = F.nll_loss(output, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "    def run_test(self):  # 执行一次测试的逻辑，与本地模型测时的代码相同\n",
    "        self.model.eval()\n",
    "        start = time()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for data, labels in self.test_loader:\n",
    "                data, labels = data.to(self.device), labels.to(self.device)\n",
    "                output: torch.Tensor = self.model(data)\n",
    "                test_loss += F.nll_loss(output, labels, reduction='sum').item()\n",
    "                pred = output.max(1, keepdim=True)[1]\n",
    "                correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "\n",
    "        end = time()\n",
    "\n",
    "        test_loss /= len(self.test_loader.dataset)\n",
    "        accuracy = correct / len(self.test_loader.dataset)\n",
    "        correct_rate = 100. * accuracy\n",
    "\n",
    "        # 记录 TensorBoard 日志，self.current_round 的值为“当前是训练的第几轮”\n",
    "        self.tb_writer.add_scalar('timer/run_time', end - start, self.current_round)\n",
    "        self.tb_writer.add_scalar('test_results/average_loss', test_loss, self.current_round)\n",
    "        self.tb_writer.add_scalar('test_results/accuracy', accuracy, self.current_round)\n",
    "        self.tb_writer.add_scalar('test_results/correct_rate', correct_rate, self.current_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df253cc9-b282-427e-b3df-e7234f25ba1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-03 08:35:02,040|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='init'\n",
      "2023-02-03 08:35:02,063|INFO|scheduler|push_log|118:\n",
      "Begin to validate local context.\n",
      "2023-02-03 08:35:02,064|INFO|scheduler|push_log|118:\n",
      "Local context is ready.\n",
      "2023-02-03 08:35:02,065|INFO|scheduler|push_log|118:\n",
      "Node 663ad4b0-b617-409f-8bc9-3682b30f7f30 is up.\n",
      "2023-02-03 08:35:02,066|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='gethering'\n",
      "2023-02-03 08:35:02,066|INFO|scheduler|push_log|118:\n",
      "Checking in the task ...\n",
      "2023-02-03 08:35:02,077|DEBUG|fed_avg|_wait_for_check_in_response|479:\n",
      "_wait_for_check_in_response ...\n",
      "2023-02-03 08:35:03,104|INFO|scheduler|push_log|118:\n",
      "Node 663ad4b0-b617-409f-8bc9-3682b30f7f30 have taken part in the task.\n",
      "2023-02-03 08:35:03,105|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='ready'\n",
      "2023-02-03 08:35:03,105|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='synchronizing'\n",
      "2023-02-03 08:35:03,106|INFO|scheduler|push_log|118:\n",
      "Synchronizing round state ...\n",
      "2023-02-03 08:35:03,107|INFO|scheduler|push_log|118:\n",
      "Waiting for synchronizing state with the aggregator ...\n",
      "2023-02-03 08:35:18,139|INFO|scheduler|push_log|118:\n",
      "Successfully synchronized state with the aggregator.\n",
      "2023-02-03 08:35:18,139|INFO|scheduler|push_log|118:\n",
      "Successfully synchronized state in round 1\n",
      "2023-02-03 08:35:18,140|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='in_a_round'\n",
      "2023-02-03 08:35:18,141|INFO|scheduler|push_log|118:\n",
      "Waiting for training of round 1 begin ...\n",
      "2023-02-03 08:35:19,183|INFO|scheduler|push_log|118:\n",
      "Training of round 1 begins.\n",
      "2023-02-03 08:35:19,184|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='updating'\n",
      "2023-02-03 08:35:19,184|INFO|scheduler|push_log|118:\n",
      "Waiting for receiving latest parameters from the aggregator ...\n",
      "2023-02-03 08:35:21,148|INFO|scheduler|push_log|118:\n",
      "Successfully received latest parameters.\n",
      "2023-02-03 08:35:21,151|INFO|scheduler|push_log|118:\n",
      "Saved latest parameters locally.\n",
      "2023-02-03 08:35:21,152|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='calculating'\n",
      "2023-02-03 08:35:21,154|INFO|scheduler|push_log|118:\n",
      "Begin to run calculation ...\n",
      "2023-02-03 08:35:36,861|INFO|scheduler|push_log|118:\n",
      "Local calculation complete.\n",
      "2023-02-03 08:35:36,862|INFO|scheduler|push_log|118:\n",
      "Waiting for aggregation begin ...\n",
      "2023-02-03 08:35:36,877|INFO|scheduler|push_log|118:\n",
      "Pushing local update to the aggregator ...\n",
      "2023-02-03 08:35:37,980|DEBUG|shared_file_data_channel|_do_send_stream|136:\n",
      "Sending data stream complete.\n",
      "2023-02-03 08:35:38,080|INFO|scheduler|push_log|118:\n",
      "Successfully pushed local update to the aggregator.\n",
      "2023-02-03 08:35:38,081|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='closing_round'\n",
      "2023-02-03 08:35:38,081|INFO|scheduler|push_log|118:\n",
      "Waiting for closing signal of training round 1 ...\n",
      "2023-02-03 08:35:40,104|INFO|scheduler|push_log|118:\n",
      "ID: 663ad4b0-b617-409f-8bc9-3682b30f7f30 finished training task of round 1.\n",
      "2023-02-03 08:35:40,105|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='ready'\n",
      "2023-02-03 08:35:40,106|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='synchronizing'\n",
      "2023-02-03 08:35:40,106|INFO|scheduler|push_log|118:\n",
      "Synchronizing round state ...\n",
      "2023-02-03 08:35:40,107|INFO|scheduler|push_log|118:\n",
      "Waiting for synchronizing state with the aggregator ...\n",
      "2023-02-03 08:35:40,120|INFO|scheduler|push_log|118:\n",
      "Successfully synchronized state with the aggregator.\n",
      "2023-02-03 08:35:40,121|INFO|scheduler|push_log|118:\n",
      "Successfully synchronized state in round 2\n",
      "2023-02-03 08:35:40,121|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='in_a_round'\n",
      "2023-02-03 08:35:40,122|INFO|scheduler|push_log|118:\n",
      "Waiting for training of round 2 begin ...\n",
      "2023-02-03 08:35:41,132|INFO|scheduler|push_log|118:\n",
      "Training of round 2 begins.\n",
      "2023-02-03 08:35:41,133|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='updating'\n",
      "2023-02-03 08:35:41,134|INFO|scheduler|push_log|118:\n",
      "Waiting for receiving latest parameters from the aggregator ...\n",
      "2023-02-03 08:35:41,970|INFO|scheduler|push_log|118:\n",
      "Successfully received latest parameters.\n",
      "2023-02-03 08:35:41,973|INFO|scheduler|push_log|118:\n",
      "Saved latest parameters locally.\n",
      "2023-02-03 08:35:41,973|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='calculating'\n",
      "2023-02-03 08:35:41,974|INFO|scheduler|push_log|118:\n",
      "Begin to run calculation ...\n",
      "2023-02-03 08:35:56,946|INFO|scheduler|push_log|118:\n",
      "Local calculation complete.\n",
      "2023-02-03 08:35:56,947|INFO|scheduler|push_log|118:\n",
      "Waiting for aggregation begin ...\n",
      "2023-02-03 08:35:56,958|INFO|scheduler|push_log|118:\n",
      "Pushing local update to the aggregator ...\n",
      "2023-02-03 08:35:58,113|DEBUG|shared_file_data_channel|_do_send_stream|136:\n",
      "Sending data stream complete.\n",
      "2023-02-03 08:35:58,164|INFO|scheduler|push_log|118:\n",
      "Successfully pushed local update to the aggregator.\n",
      "2023-02-03 08:35:58,165|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='closing_round'\n",
      "2023-02-03 08:35:58,165|INFO|scheduler|push_log|118:\n",
      "Waiting for closing signal of training round 2 ...\n",
      "2023-02-03 08:36:00,192|INFO|scheduler|push_log|118:\n",
      "ID: 663ad4b0-b617-409f-8bc9-3682b30f7f30 finished training task of round 2.\n",
      "2023-02-03 08:36:00,193|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='ready'\n",
      "2023-02-03 08:36:00,193|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='synchronizing'\n",
      "2023-02-03 08:36:00,194|INFO|scheduler|push_log|118:\n",
      "Synchronizing round state ...\n",
      "2023-02-03 08:36:00,194|INFO|scheduler|push_log|118:\n",
      "Waiting for synchronizing state with the aggregator ...\n",
      "2023-02-03 08:36:00,231|INFO|scheduler|push_log|118:\n",
      "Successfully synchronized state with the aggregator.\n",
      "2023-02-03 08:36:00,232|INFO|scheduler|push_log|118:\n",
      "Successfully synchronized state in round 3\n",
      "2023-02-03 08:36:00,233|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='in_a_round'\n",
      "2023-02-03 08:36:00,233|INFO|scheduler|push_log|118:\n",
      "Waiting for training of round 3 begin ...\n",
      "2023-02-03 08:36:01,261|INFO|scheduler|push_log|118:\n",
      "Training of round 3 begins.\n",
      "2023-02-03 08:36:01,262|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='updating'\n",
      "2023-02-03 08:36:01,263|INFO|scheduler|push_log|118:\n",
      "Waiting for receiving latest parameters from the aggregator ...\n",
      "2023-02-03 08:36:01,994|INFO|scheduler|push_log|118:\n",
      "Successfully received latest parameters.\n",
      "2023-02-03 08:36:01,997|INFO|scheduler|push_log|118:\n",
      "Saved latest parameters locally.\n",
      "2023-02-03 08:36:01,997|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='calculating'\n",
      "2023-02-03 08:36:01,998|INFO|scheduler|push_log|118:\n",
      "Begin to run calculation ...\n",
      "2023-02-03 08:36:17,895|INFO|scheduler|push_log|118:\n",
      "Local calculation complete.\n",
      "2023-02-03 08:36:17,896|INFO|scheduler|push_log|118:\n",
      "Waiting for aggregation begin ...\n",
      "2023-02-03 08:36:17,905|INFO|scheduler|push_log|118:\n",
      "Pushing local update to the aggregator ...\n",
      "2023-02-03 08:36:19,112|DEBUG|shared_file_data_channel|_do_send_stream|136:\n",
      "Sending data stream complete.\n",
      "2023-02-03 08:36:19,208|INFO|scheduler|push_log|118:\n",
      "Successfully pushed local update to the aggregator.\n",
      "2023-02-03 08:36:19,209|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='closing_round'\n",
      "2023-02-03 08:36:19,210|INFO|scheduler|push_log|118:\n",
      "Waiting for closing signal of training round 3 ...\n",
      "2023-02-03 08:36:21,222|INFO|scheduler|push_log|118:\n",
      "ID: 663ad4b0-b617-409f-8bc9-3682b30f7f30 finished training task of round 3.\n",
      "2023-02-03 08:36:21,223|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='ready'\n",
      "2023-02-03 08:36:21,223|DEBUG|scheduler|_switch_status|125:\n",
      "self.status='synchronizing'\n",
      "2023-02-03 08:36:21,225|INFO|scheduler|push_log|118:\n",
      "Synchronizing round state ...\n",
      "2023-02-03 08:36:21,225|INFO|scheduler|push_log|118:\n",
      "Waiting for synchronizing state with the aggregator ...\n",
      "2023-02-03 08:36:21,232|INFO|fed_avg|_launch_process|352:\n",
      "training task complete\n"
     ]
    }
   ],
   "source": [
    "from alphafed import mock_context\n",
    "\n",
    "# 参与方的模拟启动脚本，需要复制到另一个 Notebook 脚本文件中执行\n",
    "# scheduler 实例和聚合方的一模一样\n",
    "scheduler = DemoAvg(max_rounds=3, log_rounds=1, calculation_timeout=1800)\n",
    "\n",
    "task_id = 'cbb3ffd0-838c-41ca-a41a-7c11cae29181'  # 任务 ID 必须与聚合方一致\n",
    "aggregator_id = '1bb9feba-7b53-455b-b127-0eb19ffc9d3f'  # 必须与聚合方配置的一致\n",
    "col_id_1 = '663ad4b0-b617-409f-8bc9-3682b30f7f30'  # 必须与聚合方配置的一致\n",
    "col_id_2 = '0fc1a571-2920-47bf-9e4e-b4edb7fa2caa'  # 必须与聚合方配置的一致\n",
    "with mock_context(id=col_id_1, nodes=[aggregator_id, col_id_1, col_id_2]):  # 在模拟调试环境中运行\n",
    "    scheduler._run(id=col_id_1, task_id=task_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a26ef20-501c-4b64-84e9-a0dfd53640cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "1f0c7602c82e39efa19a01e5e068584db7a6d17aff8711ab06660aac81377393"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
