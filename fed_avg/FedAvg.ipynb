{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# åˆ›å»ºä½ çš„ç¬¬ä¸€ä¸ª FedAvg è”é‚¦å­¦ä¹ ä»»åŠ¡\n",
    "\n",
    "## æ£€æŸ¥æ•°æ®\n",
    "\n",
    "ä»»ä½•ä¸€ä¸ªå­¦ä¹ ä»»åŠ¡éƒ½éœ€è¦è¯»å…¥è®­ç»ƒæ•°æ®ï¼Œæ‰€ä»¥æˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªä»»åŠ¡æ˜¯ç¡®è®¤å‚ä¸è¿ç®—çš„èŠ‚ç‚¹æ˜¯å¦èƒ½å¤ŸæˆåŠŸçš„è®¿é—®åˆ°è®­ç»ƒæ•°æ®ã€‚ç°å®ä¸–ç•Œçš„æ•°æ®åƒå˜ä¸‡åŒ–ï¼Œä½†æ˜¯åœ¨è¾“å…¥æ¨¡å‹ä¹‹å‰ï¼Œä¸€å®šä¼šé€šè¿‡ç‰¹å¾å·¥ç¨‹æ–¹æ³•è½¬æ¢ä¸ºé€‚åˆæ¨¡å‹çš„æ•°æ®ç»“æ„ã€‚å› æ­¤æˆ‘ä»¬çš„å…³æ³¨ç‚¹ä¸»è¦æœ‰ä¸¤ä¸ªï¼š\n",
    "- æ˜¯å¦èƒ½å¤ŸæˆåŠŸè®¿é—®åˆ°è®­ç»ƒæ•°æ®ï¼Ÿ</br></br>\n",
    "åœ¨è”é‚¦å­¦ä¹ ç¯å¢ƒä¸­è®¿é—®æ•°æ®çš„æ–¹å¼ä¸åœ¨ä¸­å¿ƒåŒ–çš„ç¯å¢ƒä¸­å¹¶æ²¡æœ‰å¤šå°‘ä¸åŒã€‚åªæ˜¯ç”±äºæ•°æ®æ‹¥æœ‰è€…çš„å¤šæ ·æ€§ï¼Œéœ€è¦è€ƒè™‘å»ºç«‹ä¸€ä¸ªç»Ÿä¸€çš„è§„èŒƒï¼Œå¹¶ä¸”å°½é‡æå‡åŠ è½½æ•°æ®ä»£ç çš„å…¼å®¹æ€§ï¼Œä»¥å°½å¯èƒ½æé«˜åŠ è½½æ•°æ®çš„æˆåŠŸç‡ã€‚  \n",
    "æˆ‘ä»¬æ¨èä½ åœ¨ä»»åŠ¡æè¿°ä¸­æ˜ç¡®æŒ‡å®šæ•°æ®çš„å­˜å‚¨å’Œè®¿é—®æ–¹å¼ï¼Œä»¥å¸®åŠ©æ•°æ®æ‹¥æœ‰è€…æ˜ç¡®å¦‚ä½•å‡†å¤‡æ•°æ®ä»¥åŠåº”è¯¥å°†å®ƒä»¬å­˜æ”¾åœ¨å“ªé‡Œã€‚ï¼ˆ**éœ€è¦ä¿è¯æ•°æ®ä¾ç„¶ä¿å­˜äºæœ¬åœ°çš„å®‰å…¨ç¯å¢ƒä¸­ï¼Œå› æ­¤ä¸Šä¼ åˆ°ä¸€ä¸ªå¼€æ”¾çš„äº‘å­˜å‚¨ç¯å¢ƒå¹¶æä¾›é“¾æ¥å¯ä¸æ˜¯ä¸€ä¸ªå¥½ä¸»æ„ã€‚**ï¼‰\n",
    "\n",
    "- æ˜¯å¦èƒ½å¤ŸæˆåŠŸå°†è®­ç»ƒæ•°æ®è½¬æ¢ä¸ºé€‚åˆçš„æ•°æ®ç»“æ„ï¼Ÿ</br></br>\n",
    "åœ¨è”é‚¦å­¦ä¹ ç¯å¢ƒä¸­ï¼ŒåŸå§‹æ•°æ®çš„æ ¼å¼å¯èƒ½å­˜åœ¨ä¸€äº›å·®å¼‚ï¼Œæ¯”å¦‚å›¾ç‰‡æ•°æ®å¯èƒ½ä½¿ç”¨äº†ä¸åŒçš„å°ºå¯¸æˆ–è€…æ ¼å¼ã€‚ä¸ºäº†å°½é‡æé«˜ç‰¹å¾è½¬æ¢çš„æ­£ç¡®ç‡ï¼Œä»£ç åº”å½“å°½é‡å…¼å®¹æ›´å¤šçš„å¯èƒ½æƒ…å†µã€‚\n",
    "æˆ‘ä»¬æ¨èä½ åœ¨ä»»åŠ¡æè¿°ä¸­æ˜ç¡®æŒ‡å®šæ”¯æŒçš„æ•°æ®æ ¼å¼ï¼Œä»¥å¸®åŠ©æ•°æ®æ‹¥æœ‰è€…æ˜ç¡®å¦‚ä½•å¯¹æ•°æ®åšé¢„å¤„ç†ã€‚å¦‚æœå¯èƒ½çš„è¯ï¼Œä¹Ÿå¯ä»¥é™„å¸¦ä¸€äº›æ•°æ®å¤„ç†çš„æŒ‡å¯¼ã€‚\n",
    "\n",
    "æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªéªŒè¯è®¿é—®å’Œè½¬æ¢æ•°æ®çš„æ¨¡æ¿ï¼Œä½ å¯ä»¥åœ¨è¿™ä¸ªæ¨¡æ¿çš„åŸºç¡€ä¸Šä¿®æ”¹ï¼Œä»¥é€‚åº”ä½ çš„ä»»åŠ¡éœ€è¦ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from typing import Tuple\n",
    "\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from alphafed.scheduler import DataChecker\n",
    "\n",
    "_logger = logging.getLogger(\"app\")\n",
    "\n",
    "\n",
    "class DemoDataChecker(DataChecker):\n",
    "\n",
    "    def verify_data(self) -> bool:\n",
    "        \"\"\"æ•°æ®é›†çš„æ ¡éªŒå…·ä½“é€»è¾‘.\"\"\"\n",
    "        _logger.info(\"Dataset verification start.\")\n",
    "        root_dir = '/data/MNIST/'\n",
    "        is_touch_succ, touch_err = self._touch_data(root_dir)\n",
    "        if not is_touch_succ:\n",
    "            return is_touch_succ, touch_err\n",
    "        is_load_succ, load_err = self._load_data(root_dir)\n",
    "        if not is_load_succ:\n",
    "            return is_load_succ, load_err\n",
    "\n",
    "        return True, 'Verification Complete.'\n",
    "\n",
    "    def _touch_data(self, root_dir: str) -> Tuple[bool, str]:\n",
    "        \"\"\"æ£€æŸ¥éœ€è¦çš„æ•°æ®æ˜¯å¦å­˜åœ¨ï¼Œæ˜¯å¦èƒ½å¤Ÿè®¿é—®åˆ°.\"\"\"\n",
    "        file_list = [\n",
    "            't10k-images-idx3-ubyte',\n",
    "            't10k-images-idx3-ubyte.gz',\n",
    "            't10k-labels-idx1-ubyte',\n",
    "            't10k-labels-idx1-ubyte.gz',\n",
    "            'train-images-idx3-ubyte',\n",
    "            'train-images-idx3-ubyte.gz',\n",
    "            'train-labels-idx1-ubyte',\n",
    "            'train-labels-idx1-ubyte.gz'\n",
    "        ]\n",
    "        full_paths = [os.path.join(root_dir, _file) for _file in file_list]\n",
    "        for _file in full_paths:\n",
    "            if not os.path.exists(_file) or not os.path.isfile(_file):\n",
    "                return False, f'{_file} does not exist or is not a file.'\n",
    "\n",
    "        return True, ''\n",
    "\n",
    "    def _load_data(self, root_dir: str) -> Tuple[bool, str]:\n",
    "        \"\"\"æ£€æŸ¥éœ€è¦çš„æ•°æ®æ˜¯å¦èƒ½å¤Ÿè¢«æˆåŠŸçš„è½¬åŒ–ä¸ºè¾“å…¥å¼ é‡å’Œæ ‡ç­¾å¼ é‡.\"\"\"\n",
    "        data_loader = DataLoader(\n",
    "            torchvision.datasets.MNIST(\n",
    "                root_dir,\n",
    "                train=True,\n",
    "                download=False,\n",
    "                transform=torchvision.transforms.Compose([\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                    torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "                ])\n",
    "            )\n",
    "        )\n",
    "        if data_loader is not None and len(data_loader) > 0:\n",
    "            return True, ''\n",
    "        else:\n",
    "            return False, 'Failed to load train data.'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ‰§è¡Œæ•°æ®éªŒè¯çš„ç±»å¿…é¡»æ˜¯ `DataChecker` çš„å­ç±»ï¼Œå¹¶å®ç°å…¶ä¸­çš„ `verify_data` æ¥å£ã€‚`verify_data` æ–¹æ³•ä¸­çš„é€»è¾‘ç”¨äºæ£€æŸ¥æ˜¯å¦èƒ½å¤Ÿè®¿é—®åˆ°æ•°æ®ä»¥åŠèƒ½å¤Ÿæ­£ç¡®çš„å°†æ•°æ®è½¬åŒ–ä¸ºç‰¹å¾ï¼Œä½ å¯ä»¥æ ¹æ®ä»»åŠ¡éœ€è¦ç¼–å†™è‡ªå·±çš„æ£€æŸ¥é€»è¾‘ã€‚\n",
    "\n",
    "å®é™…ä¸Šï¼Œä¸¤æ­¥æ£€æŸ¥éƒ½ä¸æ˜¯å¿…é¡»çš„ï¼Œä»¥æä¾›æœ€å¤§çš„çµæ´»æ€§ã€‚ä½†æ˜¯æˆ‘ä»¬å¼ºçƒˆå»ºè®®ä½ å°½å¯èƒ½å®ç°å®ƒä»¬ã€‚å¦åˆ™å¯èƒ½å¯¼è‡´è¿è¡Œæ—¶é”™è¯¯ï¼Œå±Šæ—¶è°ƒè¯•çš„ä»£ä»·å¯èƒ½ä¼šé«˜å‡ºå¾ˆå¤šã€‚\n",
    "\n",
    "AlphaMed å¹³å°æä¾›ä¸¤ç§æ–¹å¼ä¸Šä¼ æ•°æ®éªŒè¯é€»è¾‘ï¼Œä¸€ç§æ˜¯åœ¨ Notebook ç¯å¢ƒä¸­å®ä¾‹åŒ– `DataChecker` å¯¹è±¡ï¼Œç„¶åè°ƒç”¨å…¶ `submit` æ¥å£ï¼Œé€‚ç”¨äºéªŒè¯é€»è¾‘ç›¸å¯¹ç®€å•ï¼Œä¸å¤ªéœ€è¦ä¾èµ–å…¶å®ƒä»£ç å’Œèµ„æºçš„æƒ…å†µã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åœ¨ Notebook ç¯å¢ƒä¸­ä¸Šä¼ æ•°æ®éªŒè¯ä»£ç \n",
    "\n",
    "# å®šä¹‰æ•°æ®éªŒè¯ä»£ç  DataChecker\n",
    "class DemoDataChecker(DataChecker):\n",
    "    ...\n",
    "\n",
    "# å®ä¾‹åŒ–å¯¹è±¡å¹¶è°ƒç”¨ submit\n",
    "checker = DemoDataChecker()\n",
    "checker.submit(task_id='YOUR_TASK_ID')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœéªŒè¯é€»è¾‘æ¯”è¾ƒå¤æ‚ï¼Œä¾èµ–çš„å¤–éƒ¨èµ„æºç›¸å¯¹è¾ƒå¤šï¼Œå¯ä»¥é‡‡ç”¨æ‰“åŒ…ä»£ç åœ¨ Playground ä¸Šä¼ çš„æ–¹å¼ã€‚æ­¤æ—¶å¿…é¡»æä¾›ä¸€ä¸ªå‘½åä¸º entry.py çš„æ–‡ä»¶ï¼Œä½œä¸ºå¯åŠ¨æ•°æ®éªŒè¯æ“ä½œçš„å…¥å£ï¼Œå¹¶åœ¨æ–‡ä»¶ä¸­æä¾›ä¸€ä¸ª `get_checker` æ–¹æ³•ï¼Œè¿”å› `DataChecker` å¯¹è±¡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ­¤å¤„å‡è®¾å°† DemoDataChecker ä»£ç ä¿å­˜åœ¨ entry.py æ–‡ä»¶ç›¸åŒç›®å½•ä¸‹çš„ data_utils.py æ–‡ä»¶ä¸­\n",
    "from data_utils import DemoDataChecker\n",
    "\n",
    "def get_checker():\n",
    "    return DemoDataChecker()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å®Œæˆåå°†æ‰€æœ‰ç›¸å…³æ–‡ä»¶æ‰“åŒ…ä¸ºä¸€ä¸ª zip æ ¼å¼çš„å‹ç¼©åŒ…ï¼ŒåŒ…æ‹¬å®ç°DataCheckerç±»çš„ data_utils.pyã€å…¥å£æ–‡ä»¶ entry.pyã€å¯é€‰çš„ä¾èµ–çš„æœ¬åœ° python æ–‡ä»¶æˆ–æ¨¡å—ã€å¯é€‰çš„ requirements.txt æ–‡ä»¶ã€‚é€šè¿‡ Playground ç•Œé¢ä¸Šä¼  zip å‹ç¼©åŒ…ã€‚\n",
    "\n",
    "## åˆ›å»ºä»»åŠ¡\n",
    "\n",
    "ç™»å½• [AlphaMed ç®¡ç†é¡µé¢](https://alphamed.ssplabs.com/)ã€‚æ–°å»ºä¸€ä¸ªè®¡ç®—ä»»åŠ¡ï¼Œåœ¨â€œè”é‚¦ç±»å‹â€é€‰é¡¹ä¸­é€‰æ‹©â€œæ¨ªå‘è”é‚¦â€ï¼Œå¹¶æ ¹æ®éœ€è¦å¡«å†™å…¶å®ƒä»»åŠ¡ä¿¡æ¯ï¼Œç„¶åç‚¹å‡»â€œåˆ›å»ºä»»åŠ¡â€ã€‚\n",
    "\n",
    "åˆ›å»ºä»»åŠ¡åï¼Œå‘èµ·æ–¹æŒ‰ç…§å‰è¿°ä¸¤ç§æ–¹å¼ä¸­çš„ä»»æ„ä¸€ç§ä¸Šä¼ æ•°æ®éªŒè¯é€»è¾‘ä»£ç ï¼Œä»»åŠ¡ç®¡ç†å™¨å°†ä½¿ç”¨è¿™äº›é€»è¾‘æ£€æŸ¥æ¯ä¸€ä¸ªå‚ä¸è®¡ç®—çš„æ•°æ®èŠ‚ç‚¹ï¼Œä»¥ç¡®ä¿è®¡ç®—ä»»åŠ¡å¯åŠ¨åæ•°æ®å¯ç”¨ã€‚ä»£ç ä¸Šä¼ æˆåŠŸåï¼Œå„ä¸ªå‚ä¸æ–¹å¯åœ¨æœ¬åœ°æ•°æ®å‡†å¤‡å¦¥å½“åé€šè¿‡ Playground è§¦å‘è‡ªå·±èŠ‚ç‚¹çš„æ•°æ®éªŒè¯æ“ä½œã€‚\n",
    "\n",
    "## å®šä¹‰ä»»åŠ¡ç»†èŠ‚\n",
    "\n",
    "ç°åœ¨ï¼Œå¯ä»¥å®šä¹‰æˆ‘ä»¬çš„è®¡ç®—ä»»åŠ¡ç»†èŠ‚äº†ã€‚æˆ‘ä»¬å‡å®šä½ å·²ç»ç†Ÿæ‚‰äº†æ·±åº¦å­¦ä¹ çš„ç›¸å…³çŸ¥è¯†ï¼Œå¹¶ä¸”å…·å¤‡äº†åˆ©ç”¨ PyTorch æ¡†æ¶å°†æ¨¡å‹æ¦‚å¿µè½¬åŒ–ä¸ºä»£ç çš„èƒ½åŠ›ã€‚è‹¥éå¦‚æ­¤ï¼Œå»ºè®®ä½ å…ˆå»å­¦ä¹ ä¸€ä¸‹è¿™äº›çŸ¥è¯†ï¼Œç„¶åå†å›æ¥ç»§ç»­åé¢çš„æ—…ç¨‹ã€‚\n",
    "\n",
    "è¦å®šä¹‰ä¸€ä¸ª FedAvg è”é‚¦å­¦ä¹ ä»»åŠ¡ï¼Œä½ éœ€è¦å®šä¹‰ä¸€ä¸ª â€œFedAvgSchedulerâ€ çš„å­ç±»ï¼Œå¹¶å®ç°å…¶ä¸­çš„ä¸€äº›æ–¹æ³•ã€‚ä½ å¯ä»¥åœ¨æˆ‘ä»¬çš„ Notebook ç¼–è¾‘æ¡†ä¸­åˆ›å»ºä½ çš„ç¬¬ä¸€ä¸ª FedAvg è”é‚¦å­¦ä¹ ä»»åŠ¡ï¼Œå°±åƒè¿™æ ·ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import time\n",
    "from typing import Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from alphafed import get_dataset_dir, logger\n",
    "from alphafed.fed_avg import FedAvgScheduler\n",
    "\n",
    "class DemoFedAvg(FedAvgScheduler):\n",
    "    ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FedAvgScheduler æ¥æ”¶ä¸€äº›åˆå§‹åŒ–å‚æ•°ï¼Œä¸€äº›ä¸»è¦çš„å‚æ•°è¯´æ˜å¦‚ä¸‹ï¼š\n",
    "- max_roundsï¼ˆå¿…å¡«å‚æ•°ï¼‰ï¼šæœ€å¤šè®­ç»ƒè½®æ¬¡ï¼Œè¾¾åˆ°æ­¤è®­ç»ƒè½®æ¬¡åä»»åŠ¡ç»“æŸï¼›\n",
    "- nameï¼ˆå¯é€‰å‚æ•°ï¼‰ï¼šè®­ç»ƒä»»åŠ¡çš„åç§°ï¼Œé»˜è®¤ä¸ºä»»åŠ¡ IDï¼›\n",
    "- merge_epochsï¼ˆå¯é€‰å‚æ•°ï¼‰ï¼šæ¯ä¸€è½®èšåˆå‚æ•°ä¹‹å‰ï¼Œåœ¨æœ¬åœ°æ‰§è¡Œå¤šå°‘ä¸ªè®­ç»ƒè½®æ¬¡ï¼Œé»˜è®¤ä¸º 1ï¼›\n",
    "- calculation_timeoutï¼ˆå¯é€‰å‚æ•°ï¼‰ï¼šè®¡ç®—è¶…æ—¶æ—¶é—´ï¼Œå¼€å§‹æ‰§è¡Œæœ¬åœ°è®­ç»ƒåï¼Œåœ¨æ­¤æ—¶é—´å†…æ²¡æœ‰æäº¤å‚æ•°æ›´æ–°ç»“æœå°†è¢«è§†ä¸ºè¶…æ—¶ï¼›\n",
    "- log_roundsï¼ˆå¯é€‰å‚æ•°ï¼‰ï¼šæ¯éš”å‡ è½®è®­ç»ƒåï¼Œæ‰§è¡Œä¸€æ¬¡æµ‹è¯•å¹¶è®°å½•å½“å‰æ¨¡å‹æµ‹è¯•ç»“æœï¼›\n",
    "\n",
    "æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬ä¸€æ­¥ä¸€æ­¥å®Œæˆå®ƒã€‚\n",
    "\n",
    "### å®šä¹‰æ¨¡å‹å’Œä¼˜åŒ–å™¨\n",
    "\n",
    "ç¬¬ä¸€ä¸ªä»»åŠ¡æ˜¯å®ç° â€œmake\\_modelâ€ æ–¹æ³•ï¼Œå®ƒå°†è¿”å›ä»»åŠ¡è®¡ç®—æ—¶ä½¿ç”¨çš„æ¨¡å‹å¯¹è±¡ã€‚å¹¸è¿çš„æ˜¯ï¼Œè¿™å°±æ˜¯ä¸ªæ™®é€šçš„ torch.nn.Module å¯¹è±¡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"in net.py\"\"\"\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=10, out_channels=20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(in_features=320, out_features=50)\n",
    "        self.fc2 = nn.Linear(in_features=50, out_features=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½ å¯ä»¥åœ¨ notebook ä¸­å®šä¹‰ç½‘ç»œï¼Œå¹¶å°†å®šä¹‰ç½‘ç»œçš„ä»£ç ä¸ DemoScheduler çš„ä»£ç æ”¾åœ¨åŒä¸€ä¸ª notebook cell ä¸­å¯åŠ¨ä»»åŠ¡ï¼›ä¹Ÿå¯ä»¥åœ¨å•ç‹¬çš„æ–‡ä»¶ä¸­å®šä¹‰ç½‘ç»œåŠç›¸å…³ç»„ä»¶ï¼Œå¹¶å°†æ‰€æœ‰ä¾èµ–çš„ä»£ç æ–‡ä»¶**æ”¾ç½®åœ¨æ§åˆ¶ä»»åŠ¡å¯åŠ¨çš„ notebook æ–‡ä»¶çš„ç›¸åŒç›®å½•ä¸‹**ã€‚æ¯”å¦‚å¯ä»¥å°† ConvNet çš„å®šä¹‰ä»£ç æ”¾ç½®åœ¨ notebook å¯åŠ¨æ–‡ä»¶åŒç›®å½•ä¸‹çš„ net.py æ–‡ä»¶ä¸­, å¹¶é€šè¿‡ import æœºåˆ¶åŠ è½½ç½‘ç»œã€‚å¦‚æœä¾èµ–çš„æ–‡ä»¶è¾ƒå¤šï¼Œéœ€è¦ä¸€å®šçš„ç›®å½•ç»“æ„ä»¥æ–¹ä¾¿ç»´æŠ¤ï¼Œå¹³å°ä¹Ÿæ”¯æŒä»»æ„å±‚æ¬¡çš„ç›®å½•ç»“æ„ã€‚ä½†éœ€è¦ç¡®ä¿**ç›®å½•çš„æ ¹èŠ‚ç‚¹å¿…é¡»æ˜¯æ§åˆ¶ä»»åŠ¡å¯åŠ¨çš„ notebook æ–‡ä»¶æ‰€åœ¨çš„ç›®å½•**ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from net import ConvNet\n",
    "\n",
    "def build_model(self) -> nn.Module:\n",
    "    model = ConvNet()\n",
    "    return model.to(self.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç„¶åï¼Œæˆ‘ä»¬æ¥ä¸ºæ¨¡å‹æ­é…ä¸€ä¸ªä¼˜åŒ–å™¨ï¼Œå¸®åŠ©æˆ‘ä»¬è‡ªåŠ¨å¤„ç†æ¢¯åº¦ä¼˜åŒ–ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(self, model: nn.Module) -> optim.Optimizer:\n",
    "    return optim.SGD(model.parameters(),\n",
    "                     lr=self.learning_rate,\n",
    "                     momentum=self.momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä»”ç»†åˆ†æä»£ç ä¼šå‘ç°ï¼Œå…¶ä¸­ä½¿ç”¨äº†ä¸¤ä¸ªè¿˜æœªå®šä¹‰çš„å¯¹è±¡å±æ€§ï¼šâ€œself.learning\\_rateâ€ï¼Œâ€œself.momentumâ€ã€‚è¿™ä¸¤ä¸ªå±æ€§æ˜¯åœ¨åˆå§‹åŒ–å¯¹è±¡çš„æ—¶å€™è¢«èµ‹å€¼çš„ï¼ŒæŒ‚åœ¨ â€œselfâ€ ä¸Šçš„å¥½å¤„æ˜¯ï¼Œå½“æˆ‘ä»¬åœ¨å…¶å®ƒæ–¹æ³•ä¸­éœ€è¦è®¿é—®å®ƒä»¬çš„æ—¶å€™ï¼Œå®ƒä»¬éšæ—¶éƒ½åœ¨ã€‚åé¢æˆ‘ä»¬ä¼šè¯¦ç»†ä»‹ç»ï¼Œç›®å‰å¯ä»¥æš‚æ—¶å¿½ç•¥è¿™ä¸ªé—®é¢˜ã€‚å½“ç„¶ä½ ä¹Ÿå¯ä»¥åœ¨ â€œmake\\_modelâ€ï¼Œâ€œmake\\_optimizerâ€ æ–¹æ³•ä¸­ç›´æ¥å®šä¹‰éœ€è¦çš„ä»»ä½•å˜é‡ã€‚\n",
    "\n",
    "### åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨\n",
    "\n",
    "ç¬¬äºŒä¸ªä»»åŠ¡æ˜¯å¤„ç†æ•°æ®çš„åŠ è½½ï¼Œéœ€è¦å®ç° â€œmake\\_train\\_dataloaderâ€ å’Œ â€œmake\\_test\\_dataloaderâ€ ä¸¤ä¸ªæ–¹æ³•ï¼Œå®ƒä»¬å°†åˆ†åˆ«è¿”å›è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®çš„æ•°æ®åŠ è½½å™¨å¯¹è±¡ã€‚åŒæ ·çš„ï¼Œè¿™ä¸¤ä¸ªå¯¹è±¡éƒ½æ˜¯æ™®é€šçš„ torch.utils.data.DataLoader å¯¹è±¡ï¼Œæˆ‘ä»¬åº”è¯¥å¯¹æ­¤å¾ˆç†Ÿæ‚‰äº†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train_dataloader(self) -> DataLoader:\n",
    "    return DataLoader(\n",
    "        torchvision.datasets.MNIST(\n",
    "            os.path.join('root_path', 'data'),\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=torchvision.transforms.Compose([\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "            ])\n",
    "        ),\n",
    "        batch_size=self.batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "def build_test_dataloader(self) -> DataLoader:\n",
    "    return DataLoader(\n",
    "        torchvision.datasets.MNIST(\n",
    "            os.path.join('root_path', 'data'),\n",
    "            train=False,\n",
    "            download=True,\n",
    "            transform=torchvision.transforms.Compose([\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "            ])\n",
    "        ),\n",
    "        batch_size=self.batch_size,\n",
    "        shuffle=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å½“ç„¶ä½ ä¹Ÿå¯ä»¥åœ¨ â€œstate\\_dictâ€ æ–¹æ³•ä¸­è¿”å›å…¶å®ƒéœ€è¦å­¦ä¹ çš„å‚æ•°ï¼Œæ¯”å¦‚ä¼˜åŒ–å™¨çš„å‚æ•°ï¼Œå¹¶åœ¨ â€œload\\_state\\_dictâ€ æ–¹æ³•ä¸­å°†å®ƒä»¬é‡æ–°åŠ è½½åˆ°ä¼˜åŒ–å™¨ä¸­ã€‚â€œstate\\_dictâ€ è¿”å›çš„æ‰€æœ‰å‚æ•°éƒ½éµå¾ª FedAvg ç®—æ³•çš„å‚æ•°å¤„ç†æµç¨‹è¿›è¡Œè®¡ç®—å’Œæ›´æ–°ï¼Œå› æ­¤è¯·ç¡®ä¿åªè¿”å›å…¼å®¹çš„å‚æ•°ç±»å‹ã€‚å¦‚æœè¿”å›äº†å¦‚ torch.bool ç±»å‹çš„å‚æ•°ï¼Œå…¶æ›´æ–°åçš„å€¼å¯èƒ½ä¸ç¬¦åˆä½ çš„é¢„æœŸï¼Œç”šè‡³å¯èƒ½å¼•èµ·ç¨‹åºé”™è¯¯ã€‚\n",
    "\n",
    "### å®šä¹‰è®­ç»ƒé€»è¾‘\n",
    "\n",
    "ç°åœ¨ï¼Œå¯ä»¥å®šä¹‰æˆ‘ä»¬çš„è®­ç»ƒè¿‡ç¨‹äº†ã€‚å¥½æ¶ˆæ¯æ˜¯ï¼Œä¸åœ¨æœ¬åœ°æ‰§è¡Œè®­ç»ƒçš„æ–¹å¼ä¸€æ¨¡ä¸€æ ·ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_an_epoch(self) -> None:\n",
    "    self.model.train()\n",
    "    for data, labels in self.train_loader:\n",
    "        data: torch.Tensor\n",
    "        labels: torch.Tensor\n",
    "        data, labels = data.to(self.device), labels.to(self.device)\n",
    "        self.optimizer.zero_grad()\n",
    "        output = self.model(data)\n",
    "        loss = F.nll_loss(output, labels)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å®šä¹‰æµ‹è¯•é€»è¾‘\n",
    "\n",
    "ä¸ºäº†éªŒè¯æ¨¡å‹çš„è®­ç»ƒæˆæœï¼Œæˆ‘ä»¬éœ€è¦åœ¨å¿…è¦çš„æ—¶å€™å¯¹å½“å‰çš„æœ€æ–°å‚æ•°åšä¸€äº›æµ‹è¯•ã€‚åŒæ ·çš„ï¼Œæµ‹è¯•çš„æ–¹å¼ä¸æœ¬åœ°æµ‹è¯•ä¹Ÿæ˜¯ä¸€æ¨¡ä¸€æ ·çš„ã€‚å¦‚æœéœ€è¦åœ¨è®­ç»ƒçš„è¿‡ç¨‹ä¸­è®°å½•ä¸€äº›è¯„ä¼°æŒ‡æ ‡ï¼Œä»¥ä¾¿è®­ç»ƒå®Œæˆä¹‹ååšä¸€äº›åˆ†æï¼Œå¯ä»¥é€šè¿‡ TensorBoard è®°å½•æ—¥å¿—æ•°æ®ã€‚ä½†æ˜¯éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå¤„äºå®‰å…¨çš„åŸå› ï¼Œå¹³å°å¯¹æ–‡ä»¶ç³»ç»Ÿçš„è®¿é—®æ˜¯æœ‰é™åˆ¶çš„ï¼Œå› æ­¤ä¸èƒ½ä¸º TensorBoard æ—¥å¿—ä»»æ„æŒ‡å®šä¿å­˜ç›®å½•ã€‚å¹³å°ä¸ºæ­¤æä¾›äº† `tb_writer` å·¥å…·ï¼Œå…¶ä¸ä»…å¯ä»¥è®°å½•æŒ‡æ ‡æ•°æ®ï¼Œè¿˜èƒ½å¤Ÿæ”¯æŒåœ¨è®­ç»ƒå®Œæˆæ—¶å¯¼å‡º TensorBoard æ—¥å¿—ã€‚ç›¸åï¼Œ**å¦‚æœéšæ„åˆ›å»º Writer å¯¹è±¡è®°å½• TensorBoard æ—¥å¿—ï¼Œå¯èƒ½å¯¼è‡´è®°å½•å¤±è´¥æˆ–æ— æ³•è®¿é—®åˆ°è®°å½•çš„æ•°æ®ã€‚**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(self):\n",
    "    start = time()\n",
    "    self.model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, labels in self.test_loader:\n",
    "            data: torch.Tensor\n",
    "            labels: torch.Tensor\n",
    "            data, labels = data.to(self.device), labels.to(self.device)\n",
    "            output: torch.Tensor = self.model(data)\n",
    "            test_loss += F.nll_loss(output, labels, reduction='sum').item()\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(self.test_loader.dataset)\n",
    "    accuracy = correct / len(self.test_loader.dataset)\n",
    "    correct_rate = 100. * accuracy\n",
    "    logger.info(f'Test set: Average loss: {test_loss:.4f}')\n",
    "    logger.info(\n",
    "        f'Test set: Accuracy: {accuracy} ({correct_rate:.2f}%)'\n",
    "    )\n",
    "\n",
    "    end = time()\n",
    "\n",
    "    self.tb_writer.add_scalar('timer/run_time', end - start, self.current_round)\n",
    "    self.tb_writer.add_scalar('test_results/average_loss', test_loss, self.current_round)\n",
    "    self.tb_writer.add_scalar('test_results/accuracy', accuracy, self.current_round)\n",
    "    self.tb_writer.add_scalar('test_results/correct_rate', correct_rate, self.current_round)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¥½äº†ï¼Œåˆ°æ­¤ä¸ºæ­¢ï¼Œæ‰€æœ‰å¿…é¡»çš„å·¥ä½œéƒ½å·²ç»åšå®Œäº†ã€‚ä¸‹é¢çš„æ­¥éª¤æ˜¯å¯é€‰çš„ï¼Œä½ å¯ä»¥æ ¹æ®è‡ªå·±çš„éœ€è¦é€‰æ‹©å®ç°ã€‚å½“ç„¶ï¼Œä¹Ÿå¯ä»¥å…¨éƒ¨è·³è¿‡ã€‚\n",
    "\n",
    "## å®šä¹‰å¯é€‰çš„ä»»åŠ¡ç»†èŠ‚\n",
    "\n",
    "### æ·»åŠ è‡ªå·±çš„åˆå§‹åŒ–é…ç½®\n",
    "\n",
    "åœ¨å¤§å¤šæ•°ç°å®åœºæ™¯ä¸­ï¼Œé»˜è®¤çš„åˆå§‹åŒ–é…ç½®é¡¹å¯èƒ½éƒ½ä¸è¶³ä»¥æ»¡è¶³ä½ çš„å…¨éƒ¨éœ€æ±‚ã€‚ä¾æ®ä½ ä½¿ç”¨çš„æ¨¡å‹å’Œè®­ç»ƒæ–¹æ³•ï¼Œä½ å¯èƒ½éœ€è¦åŠ å…¥æ›´å¤šçš„å‚æ•°ä»¥è·å¾—æ›´å¥½çš„è®­ç»ƒæ•ˆæœã€‚è¦å®ç°è¿™ä¸€ç‚¹å¾ˆç®€å•ï¼Œå°±åƒæ‰€æœ‰æ™®é€šçš„ python ç±»å®šä¹‰ä¸€æ ·ï¼Œä½ åªéœ€è¦åœ¨ â€œ\\_\\_init__â€ æ–¹æ³•ä¸­åšä¸€äº›å¿…è¦çš„å¤„ç†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self,\n",
    "             max_rounds: int = 0,\n",
    "             merge_epoch: int = 1,\n",
    "             calculation_timeout: int = 300,\n",
    "             log_rounds: int = 0,\n",
    "             batch_size: int = 64,\n",
    "             learning_rate: float = 0.01,\n",
    "             momentum: float = 0.5) -> None:\n",
    "    super().__init__(max_rounds=max_rounds,\n",
    "                     merge_epochs=merge_epoch,\n",
    "                     calculation_timeout=calculation_timeout,\n",
    "                     log_rounds=log_rounds)\n",
    "    self.batch_size = batch_size\n",
    "    self.learning_rate = learning_rate\n",
    "    self.momentum = momentum\n",
    "\n",
    "    self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    self.seed = 42\n",
    "    torch.manual_seed(self.seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿˜è®°å¾—æˆ‘ä»¬ä¹‹å‰é‡åˆ°çš„ â€œself.learning\\_rateâ€ã€â€œself.momentumâ€ å‚æ•°å—ï¼Ÿç°åœ¨ä½ çŸ¥é“å®ƒä»¬æ˜¯æ€ä¹ˆæ¥çš„äº†å§ã€‚åœ¨ç°å®åœºæ™¯ä¸­ï¼Œæˆ‘ä»¬å¯èƒ½éœ€è¦æ·»åŠ å¤šä¸ªæœ‰åŠ©äºæ¨¡å‹è®­ç»ƒçš„å‚æ•°ã€‚ä¹Ÿè®¸å…¶ä¸­æœ‰ä¸€äº›æ˜¯éœ€è¦åœ¨å¤–éƒ¨æ§åˆ¶çš„ï¼Œæ¯”å¦‚ â€œbatch\\_sizeâ€ã€â€œlearning\\_rateâ€ã€â€œmomentumâ€ï¼Œä½ å¯ä»¥æŠŠå®ƒä»¬åŠ å…¥åˆå§‹åŒ–çš„å‚æ•°åˆ—è¡¨ä¸­ã€‚ä¹Ÿè®¸è¿˜æœ‰ä¸€äº›å‚æ•°ä¸å¸Œæœ›å—åˆ°å¤–éƒ¨ç¯å¢ƒçš„å½±å“ï¼Œæ¯”å¦‚ â€œdeviceâ€ã€â€œseedâ€ï¼Œé‚£å°±æŠŠå®ƒä»¬è—åœ¨ â€œ\\_\\_init__â€ å†…éƒ¨å¥½äº†ã€‚è¿™æ ·å¯ä»¥é¿å…å¤–éƒ¨ä½¿ç”¨è€…é”™è¯¯çš„è®¾ç½®è¿™äº›å‚æ•°ï¼ŒåŒæ—¶è¿˜èƒ½ä¿è¯è®­ç»ƒè¿‡ç¨‹ä¸­éšæ—¶å¯ä»¥è®¿é—®åˆ°è¿™äº›å‚æ•°ã€‚\n",
    "\n",
    "**è¯·è®°ä½ï¼Œæ‰€æœ‰è¿™äº›é™„åŠ çš„å‚æ•°ï¼Œéƒ½éœ€è¦ä½ è‡ªå·±æ¥ç®¡ç†ã€‚**\n",
    "\n",
    "### è·å–å’ŒåŠ è½½å‚æ•°\n",
    "\n",
    "æˆ‘ä»¬çš„è®¡ç®—ç›®æ ‡æ˜¯æ›´æ–°æ¨¡å‹çš„ç›¸å…³å‚æ•°ï¼Œé»˜è®¤æƒ…å†µä¸‹ï¼Œå¹³å°ä¼šé€šè¿‡æ¨¡å‹å¯¹è±¡çš„ `state_dict` æ–¹æ³•å’Œ `load_state_dict` æ–¹æ³•è·å–å’Œæ›´æ–°æ¨¡å‹å‚æ•°ã€‚å¦‚æœè¿™ä¸ªé»˜è®¤å®ç°ä¸èƒ½å¤Ÿæ»¡è¶³å½“å‰å®é™…ä¸šåŠ¡éœ€è¦ï¼Œä¹Ÿå¯ä»¥é€šè¿‡é‡å†™è¿™ä¸¤ä¸ªæ–¹æ³•ï¼Œå®šä¹‰ä¸ªæ€§åŒ–çš„æ¨¡å‹å‚æ•°æ•°æ®ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_dict(self) -> Dict[str, torch.Tensor]:\n",
    "    return self.model.state_dict()\n",
    "\n",
    "def load_state_dict(self, state_dict: Dict[str, torch.Tensor]):\n",
    "    self.model.load_state_dict(state_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### éªŒè¯è¿è¡Œç¯å¢ƒ\n",
    "\n",
    "åœ¨å®é™…è¿è¡Œä¹‹å‰ï¼Œå¯èƒ½ä¼šå¸Œæœ›å¯¹è¿è¡Œç¯å¢ƒå†åšä¸€äº›æ£€æŸ¥ï¼Œå¸®åŠ©å‘ç°ä¸€äº›æ½œåœ¨çš„é”™è¯¯ã€‚å¦‚æœä½ ç¡®å®æœ‰æ­¤éœ€æ±‚ï¼Œå¯ä»¥å®ç° â€œvalidate\\_contextâ€ æ–¹æ³•ï¼Œæ·»åŠ ä»»ä½•ä½ éœ€è¦çš„æ£€æŸ¥é€»è¾‘ï¼Œæˆ–è€…è¾“å‡ºä¸€äº›ç¯å¢ƒä¿¡æ¯ä»¥åˆ©äºæ£€æŸ¥é—®é¢˜ã€‚ä½†æ˜¯éœ€è¦ç•™æ„ï¼Œ**ä¸è¦å¿˜è®°å…ˆè°ƒç”¨çˆ¶ç±»çš„æ–¹æ³•ï¼Œå¦åˆ™ä¼šå¯¼è‡´è¿è¡Œæ—¶é”™è¯¯ã€‚**\n",
    "\n",
    "å¦‚æœä½ æ²¡æœ‰è¿™æ–¹é¢çš„éœ€æ±‚ï¼Œåˆ™å¯ä»¥ç›´æ¥è·³è¿‡è¿™ä¸€æ­¥ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_context(self):\n",
    "    super().validate_context()\n",
    "    assert self.train_loader and len(self.train_loader) > 0, 'failed to load train data'\n",
    "    logger.info(f'There are {len(self.train_loader.dataset)} samples for training.')\n",
    "    assert self.test_loader and len(self.test_loader) > 0, 'failed to load test data'\n",
    "    logger.info(f'There are {len(self.test_loader.dataset)} samples for testing.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ§åˆ¶ä»»åŠ¡å®Œæˆçš„æ¡ä»¶\n",
    "\n",
    "é»˜è®¤æƒ…å†µä¸‹ï¼Œè®¡ç®—ä»»åŠ¡å°†åœ¨å®Œæˆ max\\_rounds è½®çš„è®­ç»ƒä¹‹åè‡ªåŠ¨å®Œæˆã€‚åœ¨ä¸€äº›æ›´å¤æ‚çš„åœºæ™¯ä¸­ï¼Œä½ å¯èƒ½å¸Œæœ›ä½¿ç”¨ä¸€äº›æ›´å¤æ‚çš„é€»è¾‘ä»¥åˆ¤æ–­æ˜¯å¦è¦ç»“æŸè®­ç»ƒï¼Œç”šè‡³å¯èƒ½å¸Œæœ›è®­ç»ƒæ°¸è¿œæ‰§è¡Œä¸‹å»ã€‚æ­¤æ—¶å°±éœ€è¦ä¿®æ”¹ â€œis\\_task\\_finishedâ€ æ–¹æ³•çš„åˆ¤æ–­é€»è¾‘äº†ï¼Œå°†å®ƒä¿®æ”¹æˆä½ å¸Œæœ›çš„æ ·å­å§ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_task_finished(self) -> bool:\n",
    "    \"\"\"By default true if reach the max rounds configured.\"\"\"\n",
    "    return self._is_reach_max_rounds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### åœ¨è¿è¡Œæ—¶åŠ¨æ€å®‰è£…ä¾èµ–çš„ç¬¬ä¸‰æ–¹æ¨¡å—\n",
    "\n",
    "å¦‚æœä½ çš„ä»£ç è¿è¡Œæ—¶ä¾èµ–äºå¹³å°æœªå®‰è£…çš„ç¬¬ä¸‰æ–¹æ¨¡å—ï¼Œä½ å¯ä»¥åœ¨æ§åˆ¶ä»»åŠ¡å¯åŠ¨çš„ notebook æ–‡ä»¶çš„ç›¸åŒç›®å½•ä¸‹æ·»åŠ ä¸€ä¸ª `requirements.txt` æ–‡ä»¶ï¼Œå¹¶åœ¨æ–‡ä»¶ä¸­ç½—åˆ—å‡ºæ‰€æœ‰éœ€è¦å®‰è£…çš„ä¾èµ–æ¨¡å—ã€‚è¿™é‡Œè¦æ³¨æ„æ–‡ä»¶åå¿…é¡»æ˜¯ `requirements.txt`ï¼Œé¿å…æ‹¼å†™é”™è¯¯ã€‚"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# in requirements.txt\n",
    "pytz==2021.1\n",
    "diskcache\n",
    "xmltodict>=0.13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æœ¬ç¤ºä¾‹ä¸­ä½¿ç”¨çš„æ–‡ä»¶çš„ç›®å½•ç»“æ„å¦‚ä¸‹ï¼š"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "â”œâ”€â”€ demo.ipynb\n",
    "â”œâ”€â”€ net.py\n",
    "â””â”€â”€ requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å¯åŠ¨ä»»åŠ¡\n",
    "\n",
    "è‡³æ­¤ï¼Œæ‰€æœ‰ä½ éœ€è¦äº†è§£çš„çŸ¥è¯†éƒ½å·²ç»ä»‹ç»å®Œäº†ã€‚åœ¨çœŸæ­£å¯åŠ¨æˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªä»»åŠ¡ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å…ˆæŠŠå‰é¢é‚£äº›é›¶æ•£çš„æ–¹æ³•å®ç°æ•´ç†ä¸€ä¸‹ï¼Œæ±‡æ€»åˆ°ä¸€èµ·ã€‚ç„¶åï¼Œä½ è¿˜éœ€è¦åœ¨ä»»åŠ¡ç®¡ç†é¡µé¢ä¸­æŸ¥çœ‹ä¸€ä¸‹å½“å‰ä»»åŠ¡çš„ IDã€‚ä»»åŠ¡ ID å¯ä»¥åœ¨ Playgroud é¡µé¢æ‰¾åˆ°å¹¶å¤åˆ¶ï¼Œå¦‚ä¸‹å›¾ï¼š\n",
    "![è·å–å½“å‰ä»»åŠ¡ ID](../resource/task_id.png)\n",
    "\n",
    "ç°åœ¨ï¼Œå‘å°„ä½ çš„ç¬¬ä¸€æš ğŸš€ å§ ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import time\n",
    "from typing import Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from alphafed import logger\n",
    "from alphafed.fed_avg import FedAvgScheduler\n",
    "\n",
    "from net import ConvNet\n",
    "\n",
    "\n",
    "class DemoFedAvg(FedAvgScheduler):\n",
    "\n",
    "    def __init__(self,\n",
    "                 max_rounds: int = 0,\n",
    "                 merge_epochs: int = 1,\n",
    "                 calculation_timeout: int = 300,\n",
    "                 log_rounds: int = 0,\n",
    "                 involve_aggregator: bool = False,\n",
    "                 batch_size: int = 64,\n",
    "                 learning_rate: float = 0.01,\n",
    "                 momentum: float = 0.5) -> None:\n",
    "        super().__init__(max_rounds=max_rounds,\n",
    "                         merge_epochs=merge_epochs,\n",
    "                         calculation_timeout=calculation_timeout,\n",
    "                         log_rounds=log_rounds,\n",
    "                         involve_aggregator=involve_aggregator)\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.seed = 42\n",
    "        torch.manual_seed(self.seed)\n",
    "\n",
    "    def build_model(self) -> nn.Module:\n",
    "        model = ConvNet()\n",
    "        return model.to(self.device)\n",
    "\n",
    "    def build_optimizer(self, model: nn.Module) -> optim.Optimizer:\n",
    "        return optim.SGD(model.parameters(),\n",
    "                        lr=self.learning_rate,\n",
    "                        momentum=self.momentum)\n",
    "\n",
    "    def build_train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            torchvision.datasets.MNIST(\n",
    "                get_dataset_dir(self.task_id),\n",
    "                train=True,\n",
    "                download=True,\n",
    "                transform=torchvision.transforms.Compose([\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                    torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "                ])\n",
    "            ),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "    def build_test_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            torchvision.datasets.MNIST(\n",
    "                get_dataset_dir(self.task_id),\n",
    "                train=False,\n",
    "                download=True,\n",
    "                transform=torchvision.transforms.Compose([\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                    torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "                ])\n",
    "            ),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "    def state_dict(self) -> Dict[str, torch.Tensor]:\n",
    "        return self.model.state_dict()\n",
    "\n",
    "    def load_state_dict(self, state_dict: Dict[str, torch.Tensor]):\n",
    "        self.model.load_state_dict(state_dict)\n",
    "\n",
    "    def validate_context(self):\n",
    "        super().validate_context()\n",
    "        assert self.train_loader and len(self.train_loader) > 0, 'failed to load train data'\n",
    "        logger.info(f'There are {len(self.train_loader.dataset)} samples for training.')\n",
    "        assert self.test_loader and len(self.test_loader) > 0, 'failed to load test data'\n",
    "        logger.info(f'There are {len(self.test_loader.dataset)} samples for testing.')\n",
    "\n",
    "    def train_an_epoch(self) -> None:\n",
    "        self.model.train()\n",
    "        for data, labels in self.train_loader:\n",
    "            data: torch.Tensor\n",
    "            labels: torch.Tensor\n",
    "            data, labels = data.to(self.device), labels.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(data)\n",
    "            loss = F.nll_loss(output, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "    def run_test(self):\n",
    "        start = time()\n",
    "        self.model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for data, labels in self.test_loader:\n",
    "                data: torch.Tensor\n",
    "                labels: torch.Tensor\n",
    "                data, labels = data.to(self.device), labels.to(self.device)\n",
    "                output: torch.Tensor = self.model(data)\n",
    "                test_loss += F.nll_loss(output, labels, reduction='sum').item()\n",
    "                pred = output.max(1, keepdim=True)[1]\n",
    "                correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "\n",
    "        test_loss /= len(self.test_loader.dataset)\n",
    "        accuracy = correct / len(self.test_loader.dataset)\n",
    "        correct_rate = 100. * accuracy\n",
    "        logger.info(f'Test set: Average loss: {test_loss:.4f}')\n",
    "        logger.info(\n",
    "            f'Test set: Accuracy: {accuracy} ({correct_rate:.2f}%)'\n",
    "        )\n",
    "\n",
    "        end = time()\n",
    "\n",
    "        self.tb_writer.add_scalar('timer/run_time', end - start, self.current_round)\n",
    "        self.tb_writer.add_scalar('test_results/average_loss', test_loss, self.current_round)\n",
    "        self.tb_writer.add_scalar('test_results/accuracy', accuracy, self.current_round)\n",
    "        self.tb_writer.add_scalar('test_results/correct_rate', correct_rate, self.current_round)\n",
    "\n",
    "\n",
    "scheduler = DemoFedAvg(max_rounds=5,\n",
    "                       merge_epochs=1,\n",
    "                       log_rounds=1,\n",
    "                       calculation_timeout=120,\n",
    "                       involve_aggregator=True)\n",
    "scheduler.submit(task_id='YOUR_TASK_ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# åˆ›å»ºä¸€ä¸ª FedSGD è”é‚¦å­¦ä¹ ä»»åŠ¡\n",
    "\n",
    "FedSGD å¯ä»¥è¢«è§†ä¸º FedAvg ç®—æ³•ä¸­çš„ä¸€ä¸ªç‰¹ä¾‹ã€‚ä¸€èˆ¬è€Œè¨€ï¼Œå®ƒçš„è®­ç»ƒé€Ÿåº¦å’Œæ•ˆæœå‡è½åäº FedAvgï¼Œå› æ­¤å¹¶ä¸é€‚åˆåº”ç”¨åœ¨å®é™…çš„ä¸šåŠ¡åœºæ™¯ä¸­ã€‚ä½†æ˜¯åœ¨ç ”ç©¶åœºæ™¯ä¸­å…¶ç»å¸¸è¢«ç”¨äºæä¾›ä¸€ä¸ªåŸºç¡€çš„ baselineï¼Œå› æ­¤æˆ‘ä»¬æä¾›äº†ä¸€ä¸ª FedSGD çš„åŸºç¡€ç±»ï¼Œä»¥æ–¹ä¾¿åœ¨ç ”ç©¶ä¸­ä½¿ç”¨ã€‚\n",
    "\n",
    "æ—¢ç„¶ FedSGD ç®—æ³•æ˜¯ FedAvg ç®—æ³•ä¸­çš„ä¸€ä¸ªç‰¹ä¾‹ï¼Œå› æ­¤äºŒè€…çš„å¤§éƒ¨åˆ†å†…å®¹æ˜¯ç›¸åŒçš„ï¼Œæ‰€ä»¥è¿™é‡Œåªä»‹ç»äºŒè€…å­˜åœ¨å·®å¼‚çš„éƒ¨åˆ†ã€‚\n",
    "\n",
    "### åˆå§‹åŒ–å‚æ•°\n",
    "\n",
    "FedSGD ç®—æ³•è¦æ±‚åœ¨ä¸€è½®è¿­ä»£ä¸­ï¼Œå¿…é¡»åŒ…å«æ‰€æœ‰çš„å‚ä¸æ–¹ï¼Œå› æ­¤åˆå§‹åŒ–å‚æ•°ä¸­çš„ max\\_clients å‚æ•°è¢«ç§»é™¤äº†ã€‚æ‰€æœ‰åœ¨çº¿çš„å‚ä¸æ–¹éƒ½ä¼šå‚ä¸æ¯ä¸€è½®çš„è¿ç®—ã€‚ä½† min\\_clients å‚æ•°ä¾ç„¶æœ‰æ•ˆã€‚\n",
    "\n",
    "FedSGD ç®—æ³•è¦æ±‚åœ¨æ¯ä¸€è½®è¿­ä»£ä¸­ï¼Œæœ¬åœ°è®­ç»ƒåªèƒ½æ‰§è¡Œä¸€ä¸ª epochï¼Œå› æ­¤ merge\\_epochs å‚æ•°è¢«ç§»é™¤äº†ï¼Œå®ƒå°†æ°¸è¿œä¸º 1ã€‚\n",
    "\n",
    "ä¸‹é¢çš„ä»£ç å±•ç¤ºäº†å¦‚ä½•å®šä¹‰ä¸€ä¸ª FedSGD ç®—æ³•çš„å®ç°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import time\n",
    "from typing import Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from alphafed import logger\n",
    "from alphafed.fed_avg import FedSGDScheduler\n",
    "\n",
    "class DemoFedSGD(FedSGDScheduler):\n",
    "    ...\n",
    "\n",
    "\n",
    "scheduler = DemoFedSGD(max_rounds=50, log_rounds=1, calculation_timeout=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ§åˆ¶è®­ç»ƒé›†çš„ batch_size\n",
    "\n",
    "FedSGD ç®—æ³•è¦æ±‚åœ¨ä¸€è½®è¿­ä»£ä¸­ï¼Œæ‰€æœ‰çš„è®­ç»ƒæ ·æœ¬åº”å½“è¢«æ”¾ç½®åœ¨ä¸€ä¸ªæ‰¹æ¬¡ä¸­ã€‚å› æ­¤åœ¨æä¾›è®­ç»ƒæ•°æ®åŠ è½½å™¨æ—¶ï¼Œè¦å°†å…¶ batch_size è®¾ç½®ä¸ºè®­ç»ƒé›†æ ·æœ¬æ€»æ•°é‡ã€‚æ¡†æ¶ä¼šå¯¹æ­¤è¿›è¡Œæ£€æŸ¥ï¼Œå¦‚æœå‘ç°ä¸ç¬¦åˆå°†ä¼šå¯¼è‡´è®­ç»ƒå¯åŠ¨å¤±è´¥ã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train_dataloader(self) -> DataLoader:\n",
    "    dataset = torchvision.datasets.MNIST(\n",
    "        os.path.join('root_path', 'data'),\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=torchvision.transforms.Compose([\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "    )\n",
    "    return DataLoader(dataset=dataset, batch_size=len(dataset), shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FedSGD ç®—æ³•å…¶ä½™éƒ¨åˆ†çš„å®ç°æ–¹å¼å’Œè¦æ±‚ä¸ FedAvg ç®—æ³•å®Œå…¨ä¸€è‡´ï¼Œè¯·å‚è€ƒ FedAvg ç®—æ³•éƒ¨åˆ†çš„è¯´æ˜ã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªå®Œæ•´çš„ FedSGD ç®—æ³•ç¤ºä¾‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from alphafed import logger\n",
    "from alphafed.fed_avg import FedSGDScheduler\n",
    "\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=10, out_channels=20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(in_features=320, out_features=50)\n",
    "        self.fc2 = nn.Linear(in_features=50, out_features=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "\n",
    "class DemoFedSGD(FedSGDScheduler):\n",
    "\n",
    "    def __init__(self,\n",
    "                 max_rounds: int = 0,\n",
    "                 calculation_timeout: int = 300,\n",
    "                 log_rounds: int = 0,\n",
    "                 batch_size: int = 64,\n",
    "                 learning_rate: float = 0.01,\n",
    "                 momentum: float = 0.5) -> None:\n",
    "        super().__init__(max_rounds=max_rounds,\n",
    "                         calculation_timeout=calculation_timeout,\n",
    "                         log_rounds=log_rounds)\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.seed = 42\n",
    "        torch.manual_seed(self.seed)\n",
    "\n",
    "    def build_model(self) -> nn.Module:\n",
    "        model = ConvNet()\n",
    "        return model.to(self.device)\n",
    "\n",
    "    def build_optimizer(self, model: nn.Module) -> optim.Optimizer:\n",
    "        return optim.SGD(model.parameters(),\n",
    "                        lr=self.learning_rate,\n",
    "                        momentum=self.momentum)\n",
    "\n",
    "    def build_train_dataloader(self) -> DataLoader:\n",
    "        dataset = torchvision.datasets.MNIST(\n",
    "            get_dataset_dir(self.task_id),\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=torchvision.transforms.Compose([\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "            ])\n",
    "        )\n",
    "        return DataLoader(dataset=dataset, batch_size=len(dataset), shuffle=True)\n",
    "\n",
    "    def build_test_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            torchvision.datasets.MNIST(\n",
    "                get_dataset_dir(self.task_id),\n",
    "                train=False,\n",
    "                download=True,\n",
    "                transform=torchvision.transforms.Compose([\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                    torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "                ])\n",
    "            ),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "    def state_dict(self) -> Dict[str, torch.Tensor]:\n",
    "        return self.model.state_dict()\n",
    "\n",
    "    def load_state_dict(self, state_dict: Dict[str, torch.Tensor]):\n",
    "        self.model.load_state_dict(state_dict)\n",
    "\n",
    "    def validate_context(self):\n",
    "        super().validate_context()\n",
    "        assert self.train_loader and len(self.train_loader) > 0, 'failed to load train data'\n",
    "        logger.info(f'There are {len(self.train_loader.dataset)} samples for training.')\n",
    "        assert self.test_loader and len(self.test_loader) > 0, 'failed to load test data'\n",
    "        logger.info(f'There are {len(self.test_loader.dataset)} samples for testing.')\n",
    "\n",
    "    def train_an_epoch(self) -> None:\n",
    "        self.model.train()\n",
    "        for data, labels in self.train_loader:\n",
    "            data: torch.Tensor\n",
    "            labels: torch.Tensor\n",
    "            data, labels = data.to(self.device), labels.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(data)\n",
    "            loss = F.nll_loss(output, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "    def run_test(self):\n",
    "        start = time()\n",
    "        self.model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for data, labels in self.test_loader:\n",
    "                data: torch.Tensor\n",
    "                labels: torch.Tensor\n",
    "                data, labels = data.to(self.device), labels.to(self.device)\n",
    "                output: torch.Tensor = self.model(data)\n",
    "                test_loss += F.nll_loss(output, labels, reduction='sum').item()\n",
    "                pred = output.max(1, keepdim=True)[1]\n",
    "                correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "\n",
    "        test_loss /= len(self.test_loader.dataset)\n",
    "        accuracy = correct / len(self.test_loader.dataset)\n",
    "        correct_rate = 100. * accuracy\n",
    "        logger.info(f'Test set: Average loss: {test_loss:.4f}')\n",
    "        logger.info(\n",
    "            f'Test set: Accuracy: {accuracy} ({correct_rate:.2f}%)'\n",
    "        )\n",
    "\n",
    "        end = time()\n",
    "\n",
    "        self.tb_writer.add_scalar('timer/run_time', end - start, self.current_round)\n",
    "        self.tb_writer.add_scalar('test_results/average_loss', test_loss, self.current_round)\n",
    "        self.tb_writer.add_scalar('test_results/accuracy', accuracy, self.current_round)\n",
    "        self.tb_writer.add_scalar('test_results/correct_rate', correct_rate, self.current_round)\n",
    "\n",
    "\n",
    "scheduler = DemoFedSGD(max_rounds=5, log_rounds=1, calculation_timeout=120)\n",
    "scheduler.submit(task_id='YOUR_TASK_ID')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "64bcadabe4cd61f3d117ba0da9d14bf2f8e35582ff79e821f2e71056f2723d1e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
