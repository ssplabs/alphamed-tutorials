{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建你的第一个 DP-FedAvg 联邦学习任务\n",
    "\n",
    "DP-FedAvg 算法是 FedAvg 算法的一个扩展，在原算法基础上提供了差分隐私的支持，以进一步保护原始数据隐私。如果你对 FedAvg 算法及其使用方式还不了解，请先移步至[创建你的第一个 FedAvg 联邦学习任务](FedAvg.ipynb)，然后再回来继续阅读。\n",
    "\n",
    "DP-FedAvg 算法是 FedAvg 算法的一个扩展，因此二者的大部分内容是相同的，所以这里只介绍二者存在差异的部分。\n",
    "\n",
    "### 初始化参数\n",
    "\n",
    "DP-FedAvg 算法在选择参与方时，是按照指定的概率 q 随机决定每一个参与方是否参加本轮次训练的，人为修改参与方会影响隐私保护和模型训练的效果。因此初始化参数中的 max\\_clients 参数被移除了。但 min\\_clients 参数依然有效。\n",
    "\n",
    "DP-FedAvg 需要一个额外的超参数 w\\_cap($\\hat\\omega$)，其值为人为设置的样本数量上界。w\\_cap 用于计算每个参与方的数据权重 $w_k \\in (0, 1]$，当参与方样本数量超过 w\\_cap 时，其数据权重达到上限 1。\n",
    "\n",
    "DP-FedAvg 需要一个额外的超参数 q $\\in (0, 1]$，用于控制参与方被选中的概率。\n",
    "\n",
    "DP-FedAvg 需要一个额外的超参数 S，用于设置隐私敏感度边界。\n",
    "\n",
    "DP-FedAvg 需要一个额外的超参数 z，用于设置噪音尺度。\n",
    "\n",
    "以下代码展示了如何定义一个 DP-FedAvg 算法的实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from alphafed import get_dataset_dir, logger\n",
    "from alphafed.fed_avg import DPFedAvgScheduler\n",
    "\n",
    "class DemoDPFedAvg(DPFedAvgScheduler):\n",
    "    ...\n",
    "\n",
    "\n",
    "scheduler = DemoDPFedAvg(w_cap=20000,\n",
    "                         q=1,\n",
    "                         S=1,\n",
    "                         z=0.1,\n",
    "                         max_rounds=5,\n",
    "                         log_rounds=1,\n",
    "                         calculation_timeout=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了满足 DP-FedAvg 算法对梯度裁剪的要求，流程需要精确控制每个 batch 数据的训练。FedAvgScheduler 中定义的 train 接口不能满足算法的要求，因此不再使用，而是由 train\\_a\\_batch 接口替代。train\\_a\\_batch 接口中只需要完成一个 batch 数据的训练，不需要完成整个训练样本的遍历。以下示例以 FedAvg 原始示例为基础，针对 DPFedAvg 算法的要求进行了改造："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_a_batch(self, *batch_train_data):\n",
    "    data: torch.Tensor\n",
    "    labels: torch.Tensor\n",
    "    data, labels = batch_train_data\n",
    "    data, labels = data.to(self.device), labels.to(self.device)\n",
    "    self.optimizer.zero_grad()\n",
    "    output = self.model(data)\n",
    "    loss = F.nll_loss(output, labels)\n",
    "    loss.backward()\n",
    "    self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "示例仅是去除了 `for data, labels in train_loader:` 的循环控制，其余逻辑维持原状。\n",
    "\n",
    "DP-FedAvg 算法其余部分的实现方式和要求与 FedAvg 算法完全一致，请参考 FedAvg 算法部分的说明。下面是一个完整的 DP-FedAvg 算法示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from alphafed import logger\n",
    "from alphafed.fed_avg import DPFedAvgScheduler\n",
    "\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=10, out_channels=20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(in_features=320, out_features=50)\n",
    "        self.fc2 = nn.Linear(in_features=50, out_features=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "\n",
    "class DemoDPFedAvg(DPFedAvgScheduler):\n",
    "\n",
    "    def __init__(self,\n",
    "                 w_cap: int,\n",
    "                 q: float,\n",
    "                 S: float,\n",
    "                 z: float,\n",
    "                 max_rounds: int = 0,\n",
    "                 merge_epoch: int = 1,\n",
    "                 calculation_timeout: int = 300,\n",
    "                 log_rounds: int = 0) -> None:\n",
    "        super().__init__(w_cap=w_cap,\n",
    "                         q=q,\n",
    "                         S=S,\n",
    "                         z=z,\n",
    "                         max_rounds=max_rounds,\n",
    "                         merge_epochs=merge_epoch,\n",
    "                         calculation_timeout=calculation_timeout,\n",
    "                         log_rounds=log_rounds)\n",
    "        self.batch_size = 64\n",
    "        self.learning_rate = 0.01\n",
    "        self.momentum = 0.5\n",
    "        self.log_interval = 5\n",
    "        self.random_seed = 42\n",
    "\n",
    "        torch.manual_seed(self.random_seed)\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    def build_model(self) -> nn.Module:\n",
    "        model = ConvNet()\n",
    "        return model.to(self.device)\n",
    "\n",
    "    def build_optimizer(self, model: nn.Module) -> optim.Optimizer:\n",
    "        return optim.SGD(model.parameters(),\n",
    "                         lr=self.learning_rate,\n",
    "                         momentum=self.momentum)\n",
    "\n",
    "    def build_train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            torchvision.datasets.MNIST(\n",
    "                get_dataset_dir(self.task_id),\n",
    "                train=True,\n",
    "                download=True,\n",
    "                transform=torchvision.transforms.Compose([\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                    torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "                ])\n",
    "            ),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "    def build_test_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            torchvision.datasets.MNIST(\n",
    "                get_dataset_dir(self.task_id),\n",
    "                train=False,\n",
    "                download=True,\n",
    "                transform=torchvision.transforms.Compose([\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                    torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "                ])\n",
    "            ),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "    def state_dict(self) -> Dict[str, torch.Tensor]:\n",
    "        return self.model.state_dict()\n",
    "\n",
    "    def load_state_dict(self, state_dict: Dict[str, torch.Tensor]):\n",
    "        self.model.load_state_dict(state_dict)\n",
    "\n",
    "    def validate_context(self):\n",
    "        super().validate_context()\n",
    "        assert self.train_loader and len(self.train_loader) > 0, 'failed to load train data'\n",
    "        self.push_log(f'There are {len(self.train_loader.dataset)} samples for training.')\n",
    "        assert self.test_loader and len(self.test_loader) > 0, 'failed to load test data'\n",
    "        self.push_log(f'There are {len(self.test_loader.dataset)} samples for testing.')\n",
    "\n",
    "    def train_a_batch(self, *batch_train_data):\n",
    "        data: torch.Tensor\n",
    "        labels: torch.Tensor\n",
    "        data, labels = batch_train_data\n",
    "        data, labels = data.to(self.device), labels.to(self.device)\n",
    "        self.optimizer.zero_grad()\n",
    "        output = self.model(data)\n",
    "        loss = F.nll_loss(output, labels)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def run_test(self):\n",
    "        self.model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for data, labels in self.test_loader:\n",
    "                data: torch.Tensor\n",
    "                labels: torch.Tensor\n",
    "                data, labels = data.to(self.device), labels.to(self.device)\n",
    "                output: torch.Tensor = self.model(data)\n",
    "                test_loss += F.nll_loss(output, labels, reduction='sum').item()\n",
    "                pred = output.max(1, keepdim=True)[1]\n",
    "                correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "\n",
    "        test_loss /= len(self.test_loader.dataset)\n",
    "        correct_rate = 100. * correct / len(self.test_loader.dataset)\n",
    "        logger.info(f'Test set: Average loss: {test_loss:.4f}')\n",
    "        logger.info(\n",
    "            f'Test set: Accuracy: {correct}/{len(self.test_loader.dataset)} ({correct_rate:.2f}%)'\n",
    "        )\n",
    "\n",
    "        self.tb_writer.add_scalar('test_results/average_loss', test_loss, self.current_round)\n",
    "        self.tb_writer.add_scalar('test_results/correct_rate', correct_rate, self.current_round)\n",
    "\n",
    "\n",
    "scheduler = DemoDPFedAvg(w_cap=20000,\n",
    "                         q=1,\n",
    "                         S=1,\n",
    "                         z=0.1,\n",
    "                         max_rounds=5,\n",
    "                         log_rounds=1,\n",
    "                         calculation_timeout=60)\n",
    "scheduler.submit(task_id='YOUR_TASK_ID')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
